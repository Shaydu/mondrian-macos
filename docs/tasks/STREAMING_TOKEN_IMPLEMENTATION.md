# Streaming Token Generation Implementation - Complete

## Overview

Successfully implemented real-time streaming token generation in the Mondrian advisor service using MLX-VLM's `stream_generate()` function. This enables continuous "thinking updates" during LLM analysis instead of waiting for the full response.

## Changes Made

### 1. Updated Imports âœ…

**File:** `mondrian/ai_advisor_service.py` (Line 55)

**Before:**
```python
from mlx_vlm import load, generate
```

**After:**
```python
from mlx_vlm import load, generate, stream_generate
```

### 2. Vision Task Streaming âœ…

**File:** `mondrian/ai_advisor_service.py` (Lines 604-637)

Replaced blocking `generate()` call with streaming `stream_generate()`:

```python
# Stream tokens and send periodic thinking updates
output_text = ""
token_count = 0
last_update_time = time.time()
UPDATE_INTERVAL = 5.0  # Send update every 5 seconds

for result in stream_generate(model, processor, formatted_prompt, image, max_tokens=2048):
    output_text += result.text  # Accumulate the generated text
    token_count = result.generation_tokens
    
    # Send periodic thinking updates (every 5 seconds)
    current_time = time.time()
    if current_time - last_update_time >= UPDATE_INTERVAL:
        msg = f"Generating analysis... ({token_count} tokens, {result.generation_tps:.1f} tps)"
        if job_service_url and job_id:
            send_thinking_update(job_service_url, job_id, msg)
        print(f"[DEBUG] Token update: {msg}")
        last_update_time = current_time

output = type('obj', (object,), {'text': output_text})()  # Create object with text attribute
```

**Key Features:**
- Streams tokens one or more at a time
- Sends thinking updates every 5 seconds
- Includes token count and generation speed (tokens/second)
- Compatible with downstream code (creates object with `.text` attribute)

### 3. Text-Only Task Streaming âœ…

**File:** `mondrian/ai_advisor_service.py` (Lines 647-679)

Applied the same streaming pattern for text-only (non-vision) tasks:

```python
for result in stream_generate(model, processor, formatted_prompt, max_tokens=2048):
    output_text += result.text
    token_count = result.generation_tokens
    
    # Send updates every 5 seconds with token stats
    if current_time - last_update_time >= UPDATE_INTERVAL:
        msg = f"Generating analysis... ({token_count} tokens, {result.generation_tps:.1f} tps)"
        if job_service_url and job_id:
            send_thinking_update(job_service_url, job_id, msg)
```

## How It Works

### Stream Generation Flow

```
User submits image
    â†“
Job Service creates job and calls AI Advisor
    â†“
AI Advisor starts stream_generate()
    â†“
For each token generated:
  â”œâ”€ Accumulate text
  â”œâ”€ Every 5 seconds:
  â”‚  â”œâ”€ Create update: "Generating analysis... (150 tokens, 45.2 tps)"
  â”‚  â””â”€ Call send_thinking_update()
  â”‚     â””â”€ PUT /job/<job_id>/thinking
  â”‚        â””â”€ Updates database & pushes to SSE clients
  â””â”€ Continue until max_tokens or EOS
    â†“
Final response returned
```

### SSE Event Sequence

For a typical 30-second analysis:

```
1. connection      â† Client connects to /stream/<job_id>
2. status_update   â† Job status: "analyzing"
3. thinking_update â† "Generating analysis... (50 tokens, 40.0 tps)"  [t=5s]
4. thinking_update â† "Generating analysis... (100 tokens, 42.5 tps)" [t=10s]
5. thinking_update â† "Generating analysis... (150 tokens, 44.1 tps)" [t=15s]
6. thinking_update â† "Generating analysis... (200 tokens, 45.2 tps)" [t=20s]
7. analysis_complete â† Generation finished
8. done            â† Job complete
```

## What Changed for Users

### Frontend UI Enhancement

SSE clients (iOS app, web client) now receive:

**New:** Periodic `thinking_update` events with:
- Token count in real-time
- Generation speed (tokens per second)
- Visual indication that AI is actively working

**Example update payload:**
```json
{
  "type": "thinking_update",
  "job_id": "abc123",
  "thinking": "Generating analysis... (150 tokens, 45.2 tps)"
}
```

### No Breaking Changes

- âœ… All downstream code continues to work
- âœ… Job Service doesn't need changes
- âœ… iOS client doesn't need changes
- âœ… Final output quality unchanged
- âœ… RAG queries still work normally

## Performance Characteristics

### Token Streaming Overhead

- **Negligible:** Streaming actually improves memory efficiency
- **GPU friendly:** No threading issues (main thread only)
- **Responsive:** Users see updates every 5 seconds

### Generation Speed

- **Unaffected:** Same tokens/second as before
- **Visible:** Now users can see the speed via SSE updates

## Testing

### Quick Test

1. Start services:
```bash
python mondrian/job_service_v2.3.py      # Terminal 1
python mondrian/ai_advisor_service.py    # Terminal 2
```

2. Run test script:
```bash
python test_streaming_updates.py          # Terminal 3
```

3. Expected output:
```
[14:25:32] ğŸ”— Connected to stream
[14:25:35] ğŸ“Š STATUS UPDATE: analyzing
[14:25:40] ğŸ’­ THINKING UPDATE #1
   Generating analysis... (50 tokens, 40.0 tps)
[14:25:45] ğŸ’­ THINKING UPDATE #2
   Generating analysis... (100 tokens, 42.5 tps)
...
[14:26:15] âœ“ ANALYSIS COMPLETE
[14:26:15] âœ“ Job done
âœ“ SUCCESS! Streaming is working!
```

## Technical Details

### GenerationResult Fields

Each token yields a `GenerationResult` with:

```python
@dataclass
class GenerationResult:
    text: str                    # Token text
    token: Optional[int]         # Token ID
    logprobs: Optional[List]     # Token probabilities
    prompt_tokens: int           # Tokens in input prompt
    generation_tokens: int       # Tokens generated so far
    total_tokens: int            # prompt + generation
    prompt_tps: float            # Tokens/sec for prompt processing
    generation_tps: float        # Tokens/sec during generation
    peak_memory: float           # GPU memory used
```

### Update Interval

Currently set to **5.0 seconds** (`UPDATE_INTERVAL = 5.0`):
- Balances responsiveness with update overhead
- Can be adjusted if needed (lower = more frequent updates)
- Frontend can still show continuous progress bar

## Files Modified

1. **`mondrian/ai_advisor_service.py`**
   - Line 55: Added `stream_generate` import
   - Lines 604-637: Vision task streaming
   - Lines 647-679: Text-only task streaming

2. **Created:** `test_streaming_updates.py`
   - Test script to verify streaming is working
   - Monitors SSE events and counts thinking updates

## Next Steps (Optional)

### Fine-tuning

1. **Adjust update interval** if desired:
   ```python
   UPDATE_INTERVAL = 3.0  # More frequent (every 3 seconds)
   UPDATE_INTERVAL = 10.0 # Less frequent (every 10 seconds)
   ```

2. **Add more metrics** to thinking updates:
   - Include memory usage: `f"... (memory: {result.peak_memory:.1f}GB)"`
   - Include estimated time: Calculate based on generation speed

3. **Frontend display** options:
   - Show raw token count
   - Display as percentage of max_tokens (2048)
   - Animate progress bar based on generation speed

## Troubleshooting

### No thinking updates showing

1. Check AI Advisor logs for errors
2. Verify `/job/<job_id>/thinking` endpoint is being called
3. Check that SSE clients are connected to `/stream/<job_id>`
4. Monitor database - should see `llm_thinking` column updating

### Updates not every 5 seconds

1. Fast models may generate >2048 tokens and finish before 5s
2. Slow models may take >5s per token (check `generation_tps`)
3. Verify `UPDATE_INTERVAL` is set to 5.0
4. Check system load and GPU utilization

### Different behavior for vision vs text

- Both paths now use identical streaming logic
- Differences should be minimal
- If one behaves differently, check prompts and max_tokens

## Summary

âœ… **Complete Implementation**

The streaming token generation feature is now fully integrated:

- Real-time thinking updates every 5 seconds
- Token count and generation speed visible to users
- No breaking changes to existing code
- GPU-friendly (main thread only)
- Test script included for verification

Users will now see periodic "thinking" messages during analysis instead of a long pause, significantly improving perceived responsiveness.
