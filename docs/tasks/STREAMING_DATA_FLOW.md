# Streaming Token Generation - Data Flow Diagram

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONDRIAN STREAMING SYSTEM                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  iOS Client  â”‚
â”‚   / Browser  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Connect to SSE Stream
       â”‚ /stream/<job_id>
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Job Service (5000)     â”‚
â”‚  â”œâ”€ /submit              â”‚
â”‚  â”œâ”€ /stream/<job_id>  â—„â”€â”€â”¼â”€â”€â”€ SSE Stream
â”‚  â””â”€ /job/<id>/thinking   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚           â”‚
       â”‚ Submit    â”‚ PUT /job/<id>/thinking
       â”‚           â–¼
       â–¼    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ AI Advisor (5100)    â”‚
  â”‚         â”œâ”€ stream_generate()   â”‚
  â”‚         â””â”€ send_thinking_updateâ”‚
  â”‚              every 5 seconds    â”‚
  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
SQLite Database
â”œâ”€ jobs table
â”‚  â”œâ”€ id
â”‚  â”œâ”€ status
â”‚  â”œâ”€ llm_thinking â—„â”€â”€â”€ Updated every 5s
â”‚  â”œâ”€ analysis_markdown
â”‚  â””â”€ ...
```

## Request/Response Timeline

```
t=0s:   User submits image
        â†“
        POST /submit
        Response: {"job_id": "abc123"}

t=0.5s: Client connects to SSE
        GET /stream/abc123
        â”œâ”€ data: {"type": "connection"}
        â””â”€ data: {"type": "status_update", "status": "analyzing"}

t=1s:   AI Advisor starts loading model

t=2s:   Model loaded, MLX stream_generate() begins
        â”œâ”€ Token 1: "The"
        â”œâ”€ Token 2: " photograph"
        â”œâ”€ Token 3: " exhibits"
        â””â”€ ... 40+ more tokens (4 seconds)

t=5s:   Token count = 50, Ready to send update
        PUT /job/abc123/thinking
        â”œâ”€ Payload: {"thinking": "Generating analysis... (50 tokens, 40.0 tps)"}
        â”œâ”€ Database updates llm_thinking
        â””â”€ SSE clients receive:
           data: {"type": "thinking_update", "thinking": "Generating analysis... (50 tokens, 40.0 tps)"}

t=6s:   Token 51, 52, 53... continue streaming

t=10s:  Token count = 100
        PUT /job/abc123/thinking
        â””â”€ SSE: "Generating analysis... (100 tokens, 42.5 tps)"

t=15s:  Token count = 150
        PUT /job/abc123/thinking
        â””â”€ SSE: "Generating analysis... (150 tokens, 44.1 tps)"

t=20s:  Generation complete (assumed max_tokens or EOS)
        â”œâ”€ Final output: full analysis text
        â”œâ”€ Database updates with final response
        â”œâ”€ SSE: {"type": "analysis_complete"}
        â””â”€ SSE: {"type": "done"}

Total time: ~20 seconds
Updates sent: 4 (every 5 seconds)
User perception: Active thinking with progress indicators
```

## Token Generation Stream Detail

```
stream_generate(model, processor, prompt, image)
â”‚
â”œâ”€ Yield GenerationResult #1
â”‚  â”œâ”€ text: "The"
â”‚  â”œâ”€ generation_tokens: 1
â”‚  â”œâ”€ generation_tps: 35.2
â”‚  â””â”€ peak_memory: 2.1 GB
â”‚
â”œâ”€ Yield GenerationResult #2
â”‚  â”œâ”€ text: " photograph"
â”‚  â”œâ”€ generation_tokens: 2
â”‚  â”œâ”€ generation_tps: 36.5
â”‚  â””â”€ peak_memory: 2.1 GB
â”‚
â”œâ”€ Yield GenerationResult #3
â”‚  â”œâ”€ text: " exhibits"
â”‚  â”œâ”€ generation_tokens: 3
â”‚  â”œâ”€ generation_tps: 37.2
â”‚  â””â”€ peak_memory: 2.1 GB
â”‚
â”œâ”€ ... (tokens 4-49) ...
â”‚
â”œâ”€ Yield GenerationResult #50
â”‚  â”œâ”€ text: "..."
â”‚  â”œâ”€ generation_tokens: 50
â”‚  â”œâ”€ generation_tps: 40.0  â—„â”€â”€â”€ UPDATE SENT HERE (every 5 seconds)
â”‚  â””â”€ peak_memory: 2.1 GB
â”‚
â””â”€ Continue until max_tokens or EOS token
```

## SSE Event Stream

```
CLIENT RECEIVES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Connection Established
   data: {"type": "connection"}
   
2. Job Status Update
   data: {"type": "status_update", "job_data": {"status": "analyzing", "current_step": "Starting advisor analysis"}}

3. First Thinking Update (t=5s)
   data: {"type": "thinking_update", "job_id": "abc123", "thinking": "Generating analysis... (50 tokens, 40.0 tps)"}
   
4. Second Thinking Update (t=10s)
   data: {"type": "thinking_update", "job_id": "abc123", "thinking": "Generating analysis... (100 tokens, 42.5 tps)"}

5. Third Thinking Update (t=15s)
   data: {"type": "thinking_update", "job_id": "abc123", "thinking": "Generating analysis... (150 tokens, 44.1 tps)"}

6. Final Analysis
   data: {"type": "analysis_complete", "job_id": "abc123", "analysis_markdown": "...full content..."}

7. Job Done
   data: {"type": "done", "job_id": "abc123", "status": "completed"}
```

## Database State Evolution

```
TIME     llm_thinking COLUMN           status      current_step
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t=0s     (NULL)                        pending     -
t=1s     "Loading model..."            analyzing   "Loading model..."
t=2s     "Generating analysis..."      analyzing   "Generating analysis..."
t=5s     "Generating analysis... (50   analyzing   "Generating analysis..."
         tokens, 40.0 tps)"
t=10s    "Generating analysis... (100  analyzing   "Generating analysis..."
         tokens, 42.5 tps)"
t=15s    "Generating analysis... (150  analyzing   "Generating analysis..."
         tokens, 44.1 tps)"
t=20s    "MLX analysis complete"       analyzing   "Finalizing..."
t=21s    (cleared)                     complete    "Analysis complete"
```

## Frontend Rendering Example (iOS)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Photo Analysis               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                 â”‚
â”‚     [Image Preview]             â”‚
â”‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Advisor: Ansel Adams            â”‚
â”‚                                 â”‚
â”‚ ğŸ’­ Generating analysis...       â”‚â—„â”€â”€ Updates every 5 seconds
â”‚    (50 tokens, 40.0 tps)        â”‚    Shows progress & speed
â”‚                                 â”‚
â”‚ â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  25%        â”‚    Progress bar (optional)
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

t=5s:  "ğŸ’­ Generating analysis... (50 tokens, 40.0 tps)"
t=10s: "ğŸ’­ Generating analysis... (100 tokens, 42.5 tps)"
t=15s: "ğŸ’­ Generating analysis... (150 tokens, 44.1 tps)"
t=20s: [Full analysis appears]

User sees active work happening instead of spinning spinner!
```

## How Endpoints Interact

```
â”Œâ”€ User Action â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                        â”‚
â”‚ POST /submit (job_service:5000)       â”‚
â”‚ â””â”€ Filename: "photo.jpg"              â”‚
â”‚ â””â”€ Advisors: ["ansel"]                â”‚
â”‚ â””â”€ Response: job_id = "abc123"        â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Job Service spawns   â”‚
         â”‚ AI Advisor process   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚                 â”‚
    â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
    â”‚        â”‚  AI Advisor    â”‚       â”‚
    â”‚        â”‚  - Load model  â”‚       â”‚
    â”‚        â”‚  - stream_gen()â”‚       â”‚
    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
    â”‚                â”‚                 â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”        â”‚
    â”‚         â”‚ Every 5 secs â”‚        â”‚
    â”‚         â”‚ PUT /job/    â”‚        â”‚
    â”‚         â”‚ abc123/      â”‚        â”‚
    â”‚         â”‚ thinking     â”‚        â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
    â”‚                â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Job Service Database        â”‚
    â”‚   - Update llm_thinking       â”‚
    â”‚   - Stream to SSE clients     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SSE Stream (/stream/abc123) â”‚
    â”‚   - Send thinking_update      â”‚
    â”‚   - To iOS/Web clients        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Comparison: Before vs After

```
BEFORE (Blocking generation):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Timeline: 0s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 20s
Action:   Submit â–¶ [Long silence..................] â–¶ Result
UI:       ğŸ”„     .........(nothing happens)........ âœ“

AFTER (Streaming generation):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Timeline: 0s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 20s
Action:   Submit â–¶ ğŸ’­ğŸ’­ğŸ’­ğŸ’­ğŸ’­ [Stream] ğŸ’­ğŸ’­ğŸ’­ â–¶ Result
UI:       ğŸ”„     ğŸ’­ ğŸ’­    ğŸ’­     ğŸ’­    ğŸ’­    âœ“
Events:   con   upd upd   upd    upd   done
          (every 5 seconds!)

User perceives: Active processing with visible progress
```

## Performance Metrics

```
GENERATION METRICS (Available in each thinking update):

generation_tokens: 150
â”œâ”€ How many tokens generated so far

generation_tps: 44.1
â”œâ”€ Tokens per second (generation speed)
â”œâ”€ Typical range: 35-50 tps on M1/M2
â”œâ”€ Slower = check GPU load
â”œâ”€ Faster = amazing!

peak_memory: 2.1
â”œâ”€ Peak GPU memory in GB
â”œâ”€ Typical: 1.5-3.0 GB for Qwen or similar

prompt_tps: 42.5
â”œâ”€ Speed of processing input tokens
â”œâ”€ Usually faster than generation

total_tokens: 180
â”œâ”€ prompt_tokens (30) + generation_tokens (150)
```

## Summary

The streaming token generation transforms the user experience from:
- **Silent wait** â†’ **Active feedback every 5 seconds**
- **Unknown duration** â†’ **Visible progress (tokens & speed)**
- **Perceived failure** â†’ **Perceived active processing**

All through a simple architectural improvement leveraging MLX-VLM's built-in `stream_generate()` function!
