[INFO] Python executable: /Users/shaydu/dev/mondrian-macos/mondrian/venv/bin/python3
[INFO] Python version: 3.12.2
[INFO] Working directory: /Users/shaydu/dev/mondrian-macos
[INFO] Environment variables:
[INFO]   MLX_USE_CPU=<not set>
[INFO]   CUDA_VISIBLE_DEVICES=<not set>
[INFO]   PYTORCH_ENABLE_MPS_FALLBACK=<not set>
[INFO] MLX default device explicitly set to: Device(gpu, 0)
[INFO] MLX configured to use GPU backend (Metal)
[INFO] MLX backend initialized successfully (GPU mode)
[INFO] AI Advisor Service v2.0-JSON starting...
[INFO] Using JSON output format (converted to HTML for backward compatibility)
[INFO] Using base64 encoding for all images
[INFO] Using MLX backend for vision analysis
[CONFIG] ANALYSIS_MODE loaded: lora (from env: lora)
[INFO] MLX Mode: ENABLED
[INFO] MLX Model: mlx-community/Qwen3-VL-4B-Instruct-4bit
[INFO] Model timeout: 300s
[INFO] RAG Service URL: http://127.0.0.1:5400
[INFO] RAG Default: ENABLED (from RAG_ENABLED env var)
[INFO] System prompt loaded from database (2751 characters)
AI Advisor Service starting up...
  Port: 5100
  DB Path: /Users/shaydu/dev/mondrian-macos/mondrian/mondrian.db
  MLX Model: mlx-community/Qwen3-VL-4B-Instruct-4bit
  Job service URL: http://127.0.0.1:5005
  Current working directory: /Users/shaydu/dev/mondrian-macos
  Image Encoding: BASE64 (all platforms)
[INFO] Initializing service with model_mode=fine_tuned
[INFO] Model Strategy: FINE-TUNED
[INFO] Loading LoRA adapter from: ./adapters/ansel
[INFO] Loading MLX model: mlx-community/Qwen3-VL-4B-Instruct-4bit...
[INFO] First load - model cache is empty, will download and load
[INFO] Testing Metal GPU access...
[INFO] ✓ Metal GPU is available and working
Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]Fetching 14 files: 100%|██████████| 14/14 [00:00<00:00, 48250.00it/s]
[INFO] MLX model loaded in 3.22s using GPU backend and cached
[INFO] MLX default device: Device(gpu, 0)
[INFO] ✓ GPU acceleration confirmed active
[INFO] Loading LoRA adapter from: ./adapters/ansel
[INFO] LoRA Config: rank=None, alpha=None, dropout=None
[INFO] Applying LoRA adapter from: ./adapters/ansel
Traceback (most recent call last):
  File "/Users/shaydu/dev/mondrian-macos/mondrian/ai_advisor_service.py", line 2523, in <module>
    success = initialize_service(
              ^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/ai_advisor_service.py", line 492, in initialize_service
    _MLX_MODEL, _MLX_PROCESSOR, _IS_FINE_TUNED = get_mlx_model(lora_path=lora_path, use_lora=True)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/ai_advisor_service.py", line 390, in get_mlx_model
    _MLX_MODEL = apply_lora_layers(_MLX_MODEL, lora_path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx_vlm/trainer/utils.py", line 153, in apply_lora_layers
    model = get_peft_model(model, list_of_modules, **config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx_vlm/trainer/utils.py", line 39, in get_peft_model
    freeze_model(model)
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx_vlm/trainer/utils.py", line 70, in freeze_model
    model[f"{name}"].freeze()
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx/nn/layers/base.py", line 507, in freeze
    self.apply_to_modules(_freeze_impl)
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx/nn/layers/base.py", line 421, in apply_to_modules
    apply_fn(prefix, mod)
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx/nn/layers/base.py", line 503, in _freeze_impl
    local_keys = m._validate_keys(local_keys, strict)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/venv/lib/python3.12/site-packages/mlx/nn/layers/base.py", line 449, in _validate_keys
    def _validate_keys(self, keys, strict):

KeyboardInterrupt
