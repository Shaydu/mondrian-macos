[INFO] Python executable: /Library/Frameworks/Python.framework/Versions/3.12/bin/python3
[INFO] Python version: 3.12.2
[INFO] Working directory: /Users/shaydu/dev/mondrian-macos
[INFO] Environment variables:
[INFO]   MLX_USE_CPU=<not set>
[INFO]   CUDA_VISIBLE_DEVICES=<not set>
[INFO]   PYTORCH_ENABLE_MPS_FALLBACK=<not set>
[INFO] MLX default device explicitly set to: Device(gpu, 0)
[INFO] MLX configured to use GPU backend (Metal)
[INFO] MLX backend initialized successfully (GPU mode)
[INFO] AI Advisor Service v2.0-JSON starting...
[INFO] Using JSON output format (converted to HTML for backward compatibility)
[INFO] Using base64 encoding for all images
[INFO] Using MLX backend for vision analysis
[INFO] MLX Mode: ENABLED
[INFO] MLX Model: mlx-community/Qwen3-VL-4B-Instruct-4bit
[INFO] Model timeout: 300s
[INFO] RAG Service URL: http://127.0.0.1:5400
[INFO] RAG Default: ENABLED (from RAG_ENABLED env var)
[INFO] System prompt loaded from database (2751 characters)
AI Advisor Service starting up...
  Port: 5100
  DB Path: /Users/shaydu/dev/mondrian-macos/mondrian/mondrian.db
  MLX Model: mlx-community/Qwen3-VL-4B-Instruct-4bit
  Job service URL: http://127.0.0.1:5005
  Current working directory: /Users/shaydu/dev/mondrian-macos
  Image Encoding: BASE64 (all platforms)
[INFO] Initializing service with model_mode=fine_tuned
[INFO] Model Strategy: FINE-TUNED
[INFO] Loading LoRA adapter from: ./adapters/ansel
[INFO] Loading MLX model: mlx-community/Qwen3-VL-4B-Instruct-4bit...
[INFO] Testing Metal GPU access...
[INFO] ✓ Metal GPU is available and working
Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]Fetching 14 files: 100%|██████████| 14/14 [00:00<00:00, 182361.04it/s]
Traceback (most recent call last):
  File "/Users/shaydu/dev/mondrian-macos/mondrian/ai_advisor_service.py", line 2494, in <module>
    success = initialize_service(
              ^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/ai_advisor_service.py", line 481, in initialize_service
    _MLX_MODEL, _MLX_PROCESSOR, _IS_FINE_TUNED = get_mlx_model(lora_path=lora_path, use_lora=True)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shaydu/dev/mondrian-macos/mondrian/ai_advisor_service.py", line 323, in get_mlx_model
    _MLX_MODEL, _MLX_PROCESSOR = load(MLX_MODEL)
                                 ^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mlx_vlm/utils.py", line 302, in load
    model = load_model(model_path, lazy, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mlx_vlm/utils.py", line 233, in load_model
    mx.eval(model.parameters())
KeyboardInterrupt
