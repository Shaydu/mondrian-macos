{
  "models": {
    "qwen3-4b-instruct": {
      "name": "Qwen3-VL-4B-Instruct",
      "model_id": "Qwen/Qwen3-VL-4B-Instruct",
      "description": "9-dimension analysis with Subject Matter scoring. Trained on 30 images including negative examples.",
      "adapter": "./adapters/ansel_qwen3_4b_full_9dim/epoch_20",
      "speed": "fast",
      "quality": "good",
      "reasoning": false,
      "tokens_per_sec": "25-35 (BF16)"
    },
    "qwen3-4b-thinking": {
      "name": "Qwen3-VL-4B-Thinking",
      "model_id": "Qwen/Qwen3-VL-4B-Thinking",
      "description": "Shows step-by-step reasoning. Great for detailed analysis with visible thinking.",
      "adapter": "./adapters/ansel_qwen3_4b_thinking/epoch_10",
      "speed": "slower",
      "quality": "excellent",
      "reasoning": true,
      "tokens_per_sec": "15-25 (BF16)"
    },
    "qwen3-8b-instruct": {
      "name": "Qwen3-VL-8B-Instruct",
      "model_id": "Qwen/Qwen3-VL-8B-Instruct",
      "description": "Larger model, better quality than 4B. Fast.",
      "adapter": "./adapters/ansel_qwen3_8b_instruct/epoch_10",
      "speed": "fast",
      "quality": "excellent",
      "reasoning": false,
      "tokens_per_sec": "20-30 (BF16)"
    },
    "qwen3-8b-thinking": {
      "name": "Qwen3-VL-8B-Thinking",
      "model_id": "Qwen/Qwen3-VL-8B-Thinking",
      "description": "Largest model with reasoning. Best quality and visible thinking.",
      "adapter": "./adapters/ansel_qwen3_8b_thinking/epoch_10",
      "speed": "slower",
      "quality": "best",
      "reasoning": true,
      "tokens_per_sec": "12-20 (BF16)"
    }
  },
  "defaults": {
    "model_preset": "qwen3-4b-instruct",
    "mode": "lora",
    "generation_profile": "optimized",
    "enable_rag": true,
    "description": "Default: Qwen3-VL-4B-Instruct with LoRA adapter, 4-beam search with 2500 tokens, RAG enabled"
  },
  "rag": {
    "max_reference_images": 3,
    "max_reference_quotes": 3,
    "description": "Maximum number of reference images and quotes to include per LLM response"
  },
  "generation_profiles": {
    "optimized": {
      "max_new_tokens": 2500,
      "num_beams": 3,
      "do_sample": false,
      "repetition_penalty": 1.2,
      "no_repeat_ngram_size": 6,
      "early_stopping": true,
      "length_penalty": 1.1,
      "description": "3-beam search with 2500 tokens, optimized for RTX 4000 Ada (20GB VRAM). Aggressive anti-repetition controls."
    },
    "fast_greedy": {
      "max_new_tokens": 1500,
      "num_beams": 1,
      "do_sample": false,
      "repetition_penalty": 1.0,
      "description": "Greedy decoding, fast inference (~2-3x faster than beam search)"
    },
    "beam_search": {
      "max_new_tokens": 3000,
      "num_beams": 5,
      "do_sample": false,
      "repetition_penalty": 1.15,
      "no_repeat_ngram_size": 4,
      "early_stopping": true,
      "length_penalty": 1.2,
      "description": "5-beam search for maximum quality and thoroughness with anti-repetition. Takes advantage of 20GB VRAM."
    },
    "sampling": {
      "max_new_tokens": 1500,
      "num_beams": 1,
      "do_sample": true,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 50,
      "repetition_penalty": 1.0,
      "description": "Nucleus sampling for more varied responses"
    },
    "ultra_fast": {
      "max_new_tokens": 1200,
      "num_beams": 1,
      "do_sample": false,
      "repetition_penalty": 1.05,
      "description": "Maximum speed: greedy decoding with minimal tokens. Best for quick iterations and testing. ~3-5x faster than beam search."
    },
    "quality_focused": {
      "max_new_tokens": 3500,
      "num_beams": 4,
      "do_sample": false,
      "temperature": 0.0,
      "repetition_penalty": 1.2,
      "early_stopping": true,
      "length_penalty": 1.3,
      "no_repeat_ngram_size": 5,
      "description": "Maximum thoughtfulness and uniqueness: 4-beam search with extended context, aggressive anti-repetition. Optimal for RTX 4000 Ada."
    }
  }
}