#!/usr/bin/env python3
"""
AI Advisor Service for Linux with NVIDIA CUDA Support
Uses PyTorch and HuggingFace Transformers for vision-language processing
Supports Qwen2-VL models with 4-bit quantization for RTX 3060
"""

import os
import sys
import json
import threading
import time
import sqlite3
import base64
import io

# Set PyTorch memory optimization to reduce fragmentation
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import logging
import argparse
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime
import traceback

# Import refactored modules
from mondrian.html_generator import (
    generate_ios_detailed_html,
    generate_summary_html,
    generate_advisor_bio_html
)
from mondrian.rag_retrieval import (
    DIMENSIONS,
    DIMENSION_TO_DB_COLUMN,
    DB_PATH,
    get_dimension_index,
    get_similar_images_from_db,
    get_images_for_weak_dimensions,
    get_top_reference_images,  # Single-pass RAG
    deduplicate_reference_images,
    get_best_image_per_dimension,
    compute_visual_relevance,
    compute_case_studies,
    get_user_dimensional_profile,
    get_images_with_embedding_retrieval,
    augment_prompt_with_rag_context,
    augment_prompt_for_pass2
)

import torch
import torch.cuda
from flask import Flask, request, jsonify
from flask_cors import CORS
from PIL import Image
import io

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

# Configure logging
from mondrian.logging_config import setup_service_logging
logger = setup_service_logging('ai_advisor_service_linux')


# ============================================================================
# Database Helper Functions
# ============================================================================

def get_config(db_path: str, key: str) -> Optional[str]:
    """Get a configuration value from the database config table"""
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT value FROM config WHERE key=?", (key,))
        row = cursor.fetchone()
        conn.close()
        if row:
            value = row[0]
            if isinstance(value, bytes):
                value = value.decode('utf-8')
            return value
        return None
    except Exception as e:
        logger.error(f"Failed to get config {key}: {e}")
        return None


def get_advisor_from_db(db_path: str, advisor_id: str) -> Optional[Dict[str, Any]]:
    """Get advisor data from database advisors table"""
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM advisors WHERE id = ?", (advisor_id,))
        row = cursor.fetchone()
        conn.close()
        if row:
            return dict(row)
        return None
    except Exception as e:
        logger.error(f"Failed to get advisor {advisor_id}: {e}")
        return None


def get_disclaimer_text(db_path: str) -> str:
    """Get disclaimer text from database config, with fallback to default"""
    disclaimer = get_config(db_path, "disclaimer")
    if disclaimer:
        return disclaimer
    
    # Fallback to default disclaimer if not in database
    return """These recommendations are generated by AI and should be used as creative guidance. Individual artistic interpretation may vary."""


class QwenAdvisor:
    """AI Advisor using Qwen2-VL or Qwen3-VL models with LoRA adapter"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2-VL-7B-Instruct", 
                 load_in_4bit: bool = True, device: Optional[str] = None,
                 adapter_path: Optional[str] = None, generation_config: Optional[Dict] = None,
                 backend: str = 'bnb'):
        """
        Initialize Qwen advisor with specified configuration
        
        Args:
            model_name: HuggingFace model ID
            load_in_4bit: Use 4-bit quantization (recommended for RTX 3060)
            device: Compute device ('cuda', 'cpu', or None for auto)
            adapter_path: Path to LoRA adapter (optional)
            generation_config: Generation parameters (max_new_tokens, temperature, etc.)
            backend: Inference backend ('bnb', 'vllm', 'awq')
        """
        self.model_name = model_name
        self.load_in_4bit = load_in_4bit
        self.adapter_path = adapter_path
        self.backend = backend.lower() if backend else 'bnb'
        self._offload_dir = None  # Track offload directory for cleanup
        
        # Store generation config with defaults
        self.generation_config = {
            "max_new_tokens": 3500,
            "num_beams": 1,
            "do_sample": False,
            "repetition_penalty": 1.0,
            "temperature": 0.7,
            "top_p": 0.95
        }
        if generation_config:
            # Filter out non-generation parameters (e.g., 'description')
            valid_gen_keys = {
                'max_new_tokens', 'num_beams', 'do_sample', 'repetition_penalty',
                'temperature', 'top_p', 'top_k', 'min_length', 'length_penalty',
                'early_stopping', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size',
                'bad_words_ids', 'force_words_ids', 'renormalize_logits', 'diversity_penalty',
                'num_beam_groups', 'diversity_penalty', 'output_scores', 'return_dict_in_generate',
                'output_hidden_states', 'output_attentions'
            }
            filtered_config = {k: v for k, v in generation_config.items() if k in valid_gen_keys}
            self.generation_config.update(filtered_config)
        
        # Determine device
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
        
        logger.info(f"Initializing Qwen advisor on {self.device}")
        logger.info(f"Model: {model_name}")
        logger.info(f"Backend: {self.backend.upper()}")
        logger.info(f"4-bit quantization: {load_in_4bit}")
        if adapter_path:
            logger.info(f"LoRA adapter: {adapter_path}")
        
        if self.device == 'cuda':
            self._log_gpu_info()
        
        self.model = None
        self.processor = None
        self._load_model()
    
    def _log_gpu_info(self):
        """Log GPU information"""
        device_name = torch.cuda.get_device_name(0)
        device_props = torch.cuda.get_device_properties(0)
        vram_gb = device_props.total_memory / (1024**3)
        
        logger.info(f"GPU Device: {device_name}")
        logger.info(f"GPU VRAM: {vram_gb:.2f} GB")
        logger.info(f"GPU Compute Capability: {device_props.major}.{device_props.minor}")
    
    def _load_model(self):
        """Load the Qwen model, processor, and optional LoRA adapter"""
        try:
            from transformers import AutoProcessor, AutoModelForCausalLM
            
            logger.info("Loading model...")
            
            # Load processor
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            
            # Detect if this is a vision-language model (Qwen2-VL, Qwen3-VL, etc.)
            # Vision-language models require AutoModelForVision2Seq instead of AutoModelForCausalLM
            is_vision_model = "VL" in self.model_name or "vision" in self.model_name.lower()
            
            if is_vision_model:
                try:
                    from transformers import AutoModelForVision2Seq
                    model_loader = AutoModelForVision2Seq
                    logger.info("Detected vision-language model, using AutoModelForVision2Seq")
                except ImportError:
                    # Fallback to AutoModelForCausalLM if Vision2Seq not available
                    model_loader = AutoModelForCausalLM
                    logger.warning("AutoModelForVision2Seq not available, falling back to AutoModelForCausalLM")
            else:
                model_loader = AutoModelForCausalLM
            
            # Use appropriate loader for model type
            if self.load_in_4bit and self.device == 'cuda':
                from transformers import BitsAndBytesConfig
                
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                
                self.model = model_loader.from_pretrained(
                    self.model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    local_files_only=False,
                    trust_remote_code=True
                )
            else:
                self.model = model_loader.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
                    device_map="auto" if self.device == 'cuda' else None,
                    low_cpu_mem_usage=True,
                    local_files_only=False,
                    trust_remote_code=True
                )
                if self.device == 'cpu':
                    self.model = self.model.to('cpu')
            
            # Enable gradient checkpointing to save memory during inference
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                logger.info("Gradient checkpointing enabled for memory efficiency")
            
            # Enable Flash Attention if available (RTX 3060+ supports it)
            try:
                if self.device == 'cuda':
                    # For PyTorch 2.0+, enable scaled_dot_product_attention with Flash Attention backend
                    self.model.config.attn_implementation = "flash_attention_2"
                    logger.info("Flash Attention 2 enabled for faster inference")
            except (AttributeError, ImportError):
                # Flash Attention not available, fall back to standard attention
                logger.debug("Flash Attention 2 not available, using standard attention")
            
            logger.info("Model loaded successfully")
            
            # Load LoRA adapter if provided
            if self.adapter_path:
                self._load_lora_adapter()
            
        except ImportError as e:
            logger.error(f"Failed to import required libraries: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise
    
    def _load_lora_adapter(self):
        """Load LoRA adapter from disk"""
        try:
            from peft import PeftModel
            from pathlib import Path
            import tempfile
            import os
            
            adapter_path = Path(self.adapter_path)
            if not adapter_path.exists():
                logger.warning(f"Adapter path does not exist: {self.adapter_path}")
                return
            
            logger.info(f"Loading LoRA adapter from {self.adapter_path}")
            
            # Create temporary offload directory for model dispatch
            offload_dir = tempfile.mkdtemp(prefix="mondrian_offload_")
            logger.info(f"Using offload directory: {offload_dir}")
            
            try:
                self.model = PeftModel.from_pretrained(
                    self.model, 
                    str(adapter_path),
                    offload_dir=offload_dir
                )
                self.model.eval()
                logger.info("LoRA adapter loaded successfully")
                
                # Store offload dir for cleanup later if needed
                self._offload_dir = offload_dir
                
            except Exception as e:
                # Cleanup offload dir if loading fails
                if os.path.exists(offload_dir):
                    import shutil
                    shutil.rmtree(offload_dir, ignore_errors=True)
                raise
            
        except Exception as e:
            logger.error(f"Failed to load LoRA adapter: {e}")
            logger.warning("Continuing with base model only")
            raise
    
    # NOTE: RAG retrieval methods moved to mondrian/rag_retrieval.py
    # Use module functions: get_similar_images_from_db, get_images_for_weak_dimensions, etc.
    
    def _deduplicate_reference_images(self, images: List[Dict[str, Any]], used_paths: set, min_images: int = 1) -> List[Dict[str, Any]]:
        """
        Remove duplicate images based on image_path to ensure each reference is used only once.
        
        Args:
            images: List of reference image dictionaries
            used_paths: Set of already used image paths
            min_images: Minimum number of images to return (will add back best images if needed)
            
        Returns:
            List of unique reference images
        """
        deduplicated = []
        for img in images:
            img_path = img.get('image_path')
            if img_path and img_path not in used_paths:
                used_paths.add(img_path)
                deduplicated.append(img)
                logger.debug(f"Added unique reference: {img.get('image_title', img_path.split('/')[-1])}")
            else:
                logger.debug(f"Skipped duplicate reference: {img.get('image_title', img_path.split('/')[-1] if img_path else 'Unknown')}")
        
        # If we have too few unique images, add back the best duplicates
        if len(deduplicated) < min_images and len(images) > len(deduplicated):
            logger.info(f"Only {len(deduplicated)} unique images found, adding back best duplicates to reach minimum of {min_images}")
            for img in images:
                if len(deduplicated) >= min_images:
                    break
                img_path = img.get('image_path')
                if img_path and img_path in used_paths:
                    # Add it back but mark it as a duplicate in the log
                    deduplicated.append(img)
                    logger.debug(f"Re-added duplicate reference: {img.get('image_title', img_path.split('/')[-1])}")
        
        return deduplicated

    def _get_best_image_per_dimension(self, advisor_id: str) -> Dict[str, Dict[str, Any]]:
        """
        Retrieve the single best reference image for EACH dimension separately.
        This ensures diversity - each dimension gets its own best exemplar.
        
        Args:
            advisor_id: Advisor to search (e.g., 'ansel')
            
        Returns:
            Dict mapping dimension name to best image for that dimension
            e.g., {'composition': {...}, 'lighting': {...}, ...}
        """
        try:
            conn = sqlite3.connect(DB_PATH)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            result = {}
            
            for dim_name in DIMENSIONS:
                db_column = DIMENSION_TO_DB_COLUMN.get(dim_name)
                if not db_column:
                    continue
                
                # Get the single best image for this dimension (score >= 8.0, ordered by score DESC)
                query = f"""
                    SELECT id, image_path, composition_score, lighting_score, 
                           focus_sharpness_score, color_harmony_score,
                           subject_isolation_score, depth_perspective_score,
                           visual_balance_score, emotional_impact_score,
                           overall_grade, image_description, image_title, date_taken, embedding
                    FROM dimensional_profiles
                    WHERE advisor_id = ?
                      AND {db_column} >= 8.0
                      AND {db_column} IS NOT NULL
                    ORDER BY {db_column} DESC
                    LIMIT 1
                """
                
                cursor.execute(query, (advisor_id,))
                row = cursor.fetchone()
                
                if row:
                    img_dict = dict(row)
                    # Don't include raw embedding in dict to avoid log clutter
                    if 'embedding' in img_dict:
                        img_dict['has_embedding'] = img_dict['embedding'] is not None
                        del img_dict['embedding']
                    result[dim_name] = img_dict
            
            conn.close()
            
            logger.info(f"[CaseStudy] Retrieved best images for {len(result)}/{len(DIMENSIONS)} dimensions")
            return result
            
        except Exception as e:
            logger.error(f"Failed to retrieve best images per dimension: {e}")
            return {}

    def _compute_visual_relevance(self, user_image_path: str, ref_image_path: str) -> float:
        """
        Compute CLIP embedding similarity between user image and reference image.
        Higher similarity = more relevant reference for the user's photo.
        
        Args:
            user_image_path: Path to user's uploaded image
            ref_image_path: Path to reference image
            
        Returns:
            Similarity score between 0.0 and 1.0
        """
        try:
            from mondrian.embedding_retrieval import compute_image_embedding, cosine_similarity
            
            user_emb = compute_image_embedding(user_image_path)
            ref_emb = compute_image_embedding(ref_image_path)
            
            if user_emb is None or ref_emb is None:
                logger.warning(f"Could not compute embeddings for relevance check")
                return 0.5  # Default to moderate relevance if we can't compute
            
            similarity = cosine_similarity(user_emb, ref_emb)
            return float(similarity)
            
        except Exception as e:
            logger.warning(f"Failed to compute visual relevance: {e}")
            return 0.5  # Default to moderate relevance on error

    def _compute_case_studies(
        self, 
        advisor_id: str,
        user_dimensions: List[Dict[str, Any]], 
        user_image_path: str = None,
        max_case_studies: int = 3,
        relevance_threshold: float = 0.25
    ) -> List[Dict[str, Any]]:
        """
        Compute which dimensions should get case studies based on:
        1. Gap between reference score and user score (larger = more learning opportunity)
        2. Visual relevance between user image and reference image
        3. Unique images only (first match wins for tie-breaking)
        
        Args:
            advisor_id: Advisor to use for reference images
            user_dimensions: List of user's dimension scores from LLM analysis
            user_image_path: Path to user's image for relevance scoring
            max_case_studies: Maximum number of case studies to include (1-3)
            relevance_threshold: Minimum visual similarity to include (0.0-1.0)
            
        Returns:
            List of case study dicts, each containing:
            - dimension_name: Which dimension this case study is for
            - user_score: User's score in this dimension
            - ref_image: Reference image dict
            - ref_score: Reference image's score in this dimension
            - gap: ref_score - user_score
            - relevance: Visual similarity score (if computed)
        """
        # Get best reference image for each dimension
        best_per_dim = self._get_best_image_per_dimension(advisor_id)
        
        if not best_per_dim:
            logger.warning("[CaseStudy] No reference images found for any dimension")
            return []
        
        # Build user score lookup (normalize dimension names)
        user_scores = {}
        for dim in user_dimensions:
            dim_name = dim.get('name', '').lower().strip()
            # Normalize common variations
            dim_name = dim_name.replace(' & ', '_').replace(' and ', '_').replace(' ', '_')
            if 'focus' in dim_name:
                dim_name = 'focus_sharpness'
            elif 'color' in dim_name:
                dim_name = 'color_harmony'
            elif 'depth' in dim_name:
                dim_name = 'depth_perspective'
            elif 'balance' in dim_name:
                dim_name = 'visual_balance'
            elif 'emotion' in dim_name or 'impact' in dim_name:
                dim_name = 'emotional_impact'
            elif 'isolation' in dim_name:
                dim_name = 'subject_isolation'
            user_scores[dim_name] = dim.get('score', 10)
        
        logger.info(f"[CaseStudy] User scores: {user_scores}")
        
        # Calculate gaps and relevance for each dimension
        candidates = []
        used_image_paths = set()
        
        for dim_name, ref_img in best_per_dim.items():
            db_column = DIMENSION_TO_DB_COLUMN.get(dim_name)
            if not db_column:
                continue
                
            ref_score = ref_img.get(db_column, 0)
            user_score = user_scores.get(dim_name, 10)
            ref_path = ref_img.get('image_path', '')
            
            # Skip if this image path already used (ensures uniqueness, first match wins)
            if ref_path in used_image_paths:
                logger.info(f"[CaseStudy] Skipping {dim_name}: image '{ref_img.get('image_title')}' already used")
                continue
            
            gap = ref_score - user_score
            
            # Only consider if there's a meaningful gap (user can learn something)
            if gap <= 0:
                logger.info(f"[CaseStudy] Skipping {dim_name}: no gap (user={user_score}, ref={ref_score})")
                continue
            
            # Compute visual relevance if user image provided
            relevance = 1.0  # Default to high relevance if no user image
            if user_image_path and ref_path and os.path.exists(ref_path):
                relevance = self._compute_visual_relevance(user_image_path, ref_path)
                logger.info(f"[CaseStudy] {dim_name}: gap={gap:.1f}, relevance={relevance:.2f}, ref='{ref_img.get('image_title')}'")
            else:
                logger.info(f"[CaseStudy] {dim_name}: gap={gap:.1f}, relevance=N/A, ref='{ref_img.get('image_title')}'")
            
            # Filter by relevance threshold
            if relevance < relevance_threshold:
                logger.info(f"[CaseStudy] Skipping {dim_name}: relevance {relevance:.2f} below threshold {relevance_threshold}")
                continue
            
            # Mark this image as used (first match wins for tie-breaking)
            used_image_paths.add(ref_path)
            
            candidates.append({
                'dimension_name': dim_name,
                'user_score': user_score,
                'ref_image': ref_img,
                'ref_score': ref_score,
                'gap': gap,
                'relevance': relevance
            })
        
        # Sort by gap descending (largest learning opportunity first)
        candidates.sort(key=lambda x: x['gap'], reverse=True)
        
        # Take top N case studies
        selected = candidates[:max_case_studies]
        
        if selected:
            selected_info = [(c['dimension_name'], f"gap={c['gap']:.1f}", c['ref_image'].get('image_title')) for c in selected]
            logger.info(f"[CaseStudy] Selected {len(selected)} case studies: {selected_info}")
        else:
            logger.info(f"[CaseStudy] No case studies selected (no gaps or low relevance)")
        
        return selected

    def _get_user_dimensional_profile(self, image_path: str) -> Dict[str, float]:
        """
        Retrieve user's dimensional profile from database if it exists.
        This allows us to do gap analysis on re-analysis or second-pass RAG.
        
        Args:
            image_path: Path to the user's image
            
        Returns:
            Dictionary of dimensional scores, or None if not found
        """
        try:
            conn = sqlite3.connect(DB_PATH)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # Get most recent profile for this image
            cursor.execute("""
                SELECT composition_score, lighting_score, focus_sharpness_score,
                       color_harmony_score, subject_isolation_score, depth_perspective_score,
                       visual_balance_score, emotional_impact_score
                FROM dimensional_profiles
                WHERE image_path = ?
                ORDER BY created_at DESC
                LIMIT 1
            """, (image_path,))
            
            row = cursor.fetchone()
            conn.close()
            
            if not row:
                return None
            
            # Convert to dictionary
            user_dims = dict(row)
            logger.info(f"Retrieved user dimensional profile for {image_path}")
            return user_dims
            
        except Exception as e:
            logger.error(f"Failed to retrieve user dimensional profile: {e}")
            return None
    
    def _get_images_with_embedding_retrieval(self, advisor_id: str, user_image_path: str, 
                                              weak_dimensions: List[str] = None, 
                                              user_scores: Dict[str, float] = None,
                                              max_images: int = 4) -> List[Dict[str, Any]]:
        """
        Retrieve reference images using CLIP visual embeddings for semantic similarity.
        Falls back to score-based retrieval if embeddings are not available.
        
        Args:
            advisor_id: Advisor to search
            user_image_path: Path to user's image for visual similarity
            weak_dimensions: User's weak dimensions for filtering
            user_scores: User's dimensional scores for gap calculation
            max_images: Maximum number of images to return
            
        Returns:
            List of reference images with base64 encoded thumbnails
        """
        try:
            # Try embedding-based retrieval first
            from mondrian.embedding_retrieval import get_images_hybrid_retrieval, get_similar_images_by_visual_embedding
            
            if user_scores and weak_dimensions:
                # Hybrid retrieval: visual similarity + dimensional gaps
                logger.info("Using hybrid embedding retrieval (visual + gap scores)")
                results = get_images_hybrid_retrieval(
                    DB_PATH, user_image_path, advisor_id,
                    weak_dimensions, user_scores, top_k=max_images
                )
            else:
                # Pure visual similarity
                logger.info("Using visual embedding retrieval")
                results = get_similar_images_by_visual_embedding(
                    DB_PATH, user_image_path, advisor_id,
                    weak_dimensions, top_k=max_images
                )
            
            if not results:
                logger.info("No embedding results, falling back to score-based retrieval")
                return get_images_for_weak_dimensions(DB_PATH, advisor_id, weak_dimensions, max_images)
            
            # Encode images as URLs instead of base64
            encoded_results = []
            for img in results:
                image_path = img.get('image_path')
                if image_path and os.path.exists(image_path):
                    try:
                        img_filename = os.path.basename(image_path)
                        img['image_url'] = f"/api/reference-image/{img_filename}"
                        img['image_filename'] = img_filename
                        encoded_results.append(img)
                    except Exception as e:
                        logger.warning(f"Failed to process image {image_path}: {e}")
            
            logger.info(f"Retrieved {len(encoded_results)} images via embedding retrieval with URLs")
            return encoded_results
            
        except ImportError as e:
            logger.warning(f"Embedding retrieval not available: {e}")
            logger.info("Falling back to score-based retrieval")
            return get_images_for_weak_dimensions(DB_PATH, advisor_id, weak_dimensions, max_images)
        except Exception as e:
            logger.error(f"Embedding retrieval failed: {e}")
            return get_images_for_weak_dimensions(DB_PATH, advisor_id, weak_dimensions, max_images)
    
    def _augment_prompt_with_rag_context(self, prompt: str, advisor_id: str, user_dimensions: Dict[str, float] = None, user_image_path: str = None) -> str:
        """
        Augment the prompt with RAG context from reference images.
        If user_dimensions are provided, finds images that excel in the user's weakest areas.
        Otherwise, uses top-rated reference images.
        
        Args:
            prompt: Original prompt
            advisor_id: Advisor to search for reference images
            user_dimensions: Optional dict of user's dimensional scores for gap analysis
            
        Returns:
            Augmented prompt with RAG context
        """
        
        # Track used images to prevent duplicates
        used_image_paths = set()
        
        # If user dimensions are provided, do gap-based analysis
        if user_dimensions:
            # Find the user's 3 weakest dimensions
            dimension_scores = []
            for dim_name, score in user_dimensions.items():
                if score is not None and dim_name.endswith('_score'):
                    clean_name = dim_name.replace('_score', '')
                    dimension_scores.append((clean_name, score))
            
            # Sort by score ascending (weakest first)
            dimension_scores.sort(key=lambda x: x[1])
            weak_dimensions = [name for name, score in dimension_scores[:3]]
            
            logger.info(f"User's weakest dimensions: {weak_dimensions}")
            
            # Try embedding-based retrieval if user image path is available
            if user_image_path:
                logger.info("Using embedding-based retrieval for visually similar references")
                reference_images = self._get_images_with_embedding_retrieval(
                    advisor_id, user_image_path, weak_dimensions, user_dimensions, max_images=4
                )
            else:
                # Fall back to score-based retrieval
                reference_images = get_images_for_weak_dimensions(DB_PATH, advisor_id, weak_dimensions, max_images=4)
            
            # Log what we got before deduplication
            logger.info(f"Retrieved {len(reference_images)} reference images before deduplication")
            if reference_images:
                titles = [img.get('image_title', 'Unknown') for img in reference_images]
                logger.info(f"Images before dedup: {titles}")
            
            # Deduplicate images
            reference_images = self._deduplicate_reference_images(reference_images, used_image_paths, min_images=2)
            
            # Log after deduplication
            logger.info(f"After deduplication: {len(reference_images)} unique images")
            if reference_images:
                titles = [img.get('image_title', 'Unknown') for img in reference_images]
                logger.info(f"Images after dedup: {titles}")
            
            if not reference_images:
                logger.info("No targeted reference images found for weak dimensions - skipping RAG augmentation")
                return prompt, []
            
            # Build targeted RAG context
            rag_context = "\n\n### TARGETED REFERENCE IMAGES FOR IMPROVEMENT:\n"
            rag_context += f"Based on the analysis, here are reference images that excel in the weakest areas ({', '.join(weak_dimensions)}).\n"
            
            # Add note about visual similarity if embedding retrieval was used
            if user_image_path and any('visual_similarity' in img or 'hybrid_score' in img for img in reference_images):
                rag_context += "These images are also visually similar to your photograph, making them excellent study references.\n"
            else:
                rag_context += "Study how these master works demonstrate excellence in the dimensions where improvement is most needed.\n"
            
        else:
            # No user dimensions yet - try visual embedding retrieval first
            if user_image_path:
                logger.info("Using visual embedding retrieval for first-time analysis")
                reference_images = self._get_images_with_embedding_retrieval(
                    advisor_id, user_image_path, weak_dimensions=None, 
                    user_scores=None, max_images=3
                )
            
            # Fall back to score-based if no embedding results
            if not reference_images:
                reference_images = self._get_similar_images_from_db(advisor_id, top_k=3)
            
            # Deduplicate images
            reference_images = self._deduplicate_reference_images(reference_images, used_image_paths, min_images=2)
            
            if not reference_images:
                logger.info("No unique reference images found after deduplication - skipping RAG augmentation")
                return prompt, []
            
            # Build RAG context with reference image names and dimensional comparisons
            rag_context = "\n\n### REFERENCE IMAGES FOR COMPARATIVE ANALYSIS:\n"
            if user_image_path and any('visual_similarity' in img for img in reference_images):
                rag_context += "These master works are visually similar to your photograph and provide dimensional benchmarks.\n"
            else:
                rag_context += "These master works from the advisor's portfolio provide dimensional benchmarks.\n"
        
        # Add reference image details with case study containers
        for i, img in enumerate(reference_images, 1):
            # Use image_title (metadata name) if available, otherwise extract filename
            img_title = img.get('image_title')
            if not img_title:
                img_path = img.get('image_path', '')
                img_title = img_path.split('/')[-1] if img_path else f"Reference {i}"
            
            # Add year if available
            year = img.get('date_taken')
            if year and str(year).strip():
                img_title_with_year = f"{img_title} ({year})"
            else:
                img_title_with_year = img_title
            
            # Get image URL for inline display
            img_filename = os.path.basename(img.get('image_path', ''))
            img_url = f"/api/reference-image/{img_filename}" if img_filename else ""
            
            # Build case study container with inline image
            rag_context += f"""
<div class="case-study-container" style="
    background: #1c1c1e; 
    border-radius: 12px; 
    padding: 20px; 
    margin: 20px 0;
    border-left: 4px solid #30b0c0;
">
    <h3 style="color: #ffffff; margin-top: 0; margin-bottom: 16px; font-size: 18px;">
        Case Study #{i}: {img_title_with_year}
    </h3>
    
    <div style="display: flex; flex-direction: column; gap: 16px;">
"""
            
            # Add image if available
            if img_url:
                rag_context += f"""
        <img src="{img_url}" style="
            width: 100%; 
            max-width: 100%; 
            height: auto; 
            border-radius: 8px; 
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        " alt="{img_title}" />
"""
            
            rag_context += """
        <div style="color: #d1d1d6; line-height: 1.6;">
"""
            
            # Add description
            if img.get('image_description'):
                rag_context += f"<p style='margin: 0 0 12px 0;'><strong>Description:</strong> {img['image_description']}</p>"
            
            # Add dimensional profile with ALL 8 dimensions
            all_dims = ['composition_score', 'lighting_score', 'focus_sharpness_score', 
                       'color_harmony_score', 'subject_isolation_score', 'depth_perspective_score',
                       'visual_balance_score', 'emotional_impact_score']
            
            if any(img.get(k) is not None for k in all_dims):
                rag_context += "<p style='margin: 0;'><strong>Technical Excellence:</strong> "
                scores = []
                
                dim_labels = {
                    'composition_score': 'Composition',
                    'lighting_score': 'Lighting',
                    'focus_sharpness_score': 'Focus & Sharpness',
                    'color_harmony_score': 'Color Harmony',
                    'subject_isolation_score': 'Subject Isolation',
                    'depth_perspective_score': 'Depth & Perspective',
                    'visual_balance_score': 'Visual Balance',
                    'emotional_impact_score': 'Emotional Impact'
                }
                
                for dim_key, dim_label in dim_labels.items():
                    if img.get(dim_key) is not None:
                        scores.append(f"{dim_label} {img[dim_key]}/10")
                
                rag_context += ", ".join(scores)
                if img.get('overall_grade'):
                    rag_context += f" (Grade: {img['overall_grade']})"
                rag_context += "</p>"
            
            rag_context += """
        </div>
    </div>
</div>
"""
            rag_context += "\n"
        
        # Add structured list of available reference images for case_studies field
        if reference_images:
            rag_context += "\n### AVAILABLE REFERENCE IMAGES FOR case_studies FIELD:\n"
            rag_context += "Use ONLY these images in your case_studies output (select 0-3 that are strong in user's weak areas):\n\n"
            
            dim_map = {
                'composition_score': 'Composition',
                'lighting_score': 'Lighting', 
                'focus_sharpness_score': 'Focus & Sharpness',
                'color_harmony_score': 'Color Harmony',
                'subject_isolation_score': 'Subject Isolation',
                'depth_perspective_score': 'Depth & Perspective',
                'visual_balance_score': 'Visual Balance',
                'emotional_impact_score': 'Emotional Impact'
            }
            
            for img in reference_images[:3]:  # Limit to 3 max
                img_title = img.get('image_title') or img.get('image_path', '').split('/')[-1]
                year = img.get('date_taken', 'Unknown')
                
                # List dimensions where this image excels (score >= 8)
                strong_dims = []
                for dim_key, dim_name in dim_map.items():
                    score = img.get(dim_key)
                    if score is not None and score >= 8:
                        strong_dims.append(f"{dim_name}({score})")
                
                if strong_dims:
                    rag_context += f"- \"{img_title}\" ({year}): Excels in [{', '.join(strong_dims)}]\n"
        
        rag_context += "\n**FOR case_studies OUTPUT:** Only cite images from the list above. Match reference strengths (>=8) to user weaknesses (<=5).\n"
        
        # Augment prompt
        augmented_prompt = f"{prompt}\n{rag_context}"
        logger.info(f"Augmented prompt with RAG context ({len(rag_context)} chars, {len(reference_images)} unique references)")
        
        # Log image titles for debugging duplication
        if reference_images:
            image_titles = [img.get('image_title', img.get('image_path', '').split('/')[-1]) for img in reference_images]
            logger.info(f"Reference images used: {image_titles}")
        
        return augmented_prompt, reference_images
    
    def analyze_image(self, image_path: str, advisor: str = "ansel", 
                     mode: str = "baseline") -> Dict[str, Any]:
        """
        Analyze an image and return structured insights
        
        Args:
            image_path: Path to image file
            advisor: Photography advisor persona (e.g., 'ansel')
            mode: Analysis mode ('baseline', 'rag', 'lora', 'lora+rag', 'rag_lora')
        
        Returns:
            Dictionary with analysis results
        """
        try:
            # Load and validate image
            image = Image.open(image_path).convert('RGB')
            logger.info(f"Loaded image: {image_path} ({image.size})")
            
            # Create analysis prompt
            prompt = self._create_prompt(advisor, mode)
            
            # Apply RAG augmentation if requested
            reference_images = []
            if mode in ('rag', 'rag_lora', 'lora+rag', 'lora_rag'):
                logger.info(f"Augmenting prompt with RAG context for mode={mode}")
                
                # Try to get user's existing dimensional profile for gap-based RAG
                user_dims = self._get_user_dimensional_profile(image_path)
                if user_dims:
                    logger.info("Using gap-based RAG with user's existing dimensional profile and visual embeddings")
                    prompt, reference_images = self._augment_prompt_with_rag_context(
                        prompt, advisor, user_dimensions=user_dims, user_image_path=image_path
                    )
                else:
                    logger.info("No existing user profile found, using standard RAG with visual embeddings")
                    prompt, reference_images = self._augment_prompt_with_rag_context(
                        prompt, advisor, user_image_path=image_path
                    )
            
            # Use chat template for proper image token handling
            messages = [
                {"role": "user", "content": [
                    {"type": "image"},
                    {"type": "text", "text": prompt}
                ]}
            ]
            
            # Prepare inputs using processor with chat template
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=text,
                images=[image],
                padding=True,
                return_tensors="pt"
            )
            
            # Move to device
            if self.device == 'cuda':
                inputs = {k: v.cuda() if hasattr(v, 'cuda') else v for k, v in inputs.items()}
            
            # Generate response
            logger.info("Running inference...")
            logger.info(f"Generation config: {self.generation_config}")
            with torch.no_grad():
                output_ids = self.model.generate(
                    **inputs, 
                    **self.generation_config,
                    eos_token_id=self.processor.tokenizer.eos_token_id
                )
            
            # Decode only the generated tokens (exclude input prompt)
            input_length = inputs['input_ids'].shape[1]
            generated_ids = output_ids[:, input_length:]
            
            response = self.processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0]
            
            logger.info(f"Generated response: {len(response)} chars")
            
            # Parse response into structured format
            analysis = self._parse_response(
                response, advisor, mode, prompt, 
                reference_images=reference_images,
                user_image_path=image_path
            )
            
            return analysis
            
        except FileNotFoundError:
            logger.error(f"Image file not found: {image_path}")
            raise
        except Exception as e:
            logger.error(f"Error analyzing image: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _create_prompt(self, advisor: str, mode: str) -> str:
        """Create analysis prompt by loading from database"""
        
        # Load system prompt from config table
        system_prompt = get_config(DB_PATH, "system_prompt")
        if not system_prompt:
            logger.warning("No system_prompt in database, using default")
            system_prompt = self._get_default_system_prompt()
        
        # Load advisor-specific prompt from advisors table
        advisor_data = get_advisor_from_db(DB_PATH, advisor)
        advisor_prompt = ""
        if advisor_data and advisor_data.get('prompt'):
            advisor_prompt = advisor_data['prompt']
            logger.info(f"Loaded advisor prompt for '{advisor}' ({len(advisor_prompt)} chars)")
        else:
            logger.warning(f"No prompt found for advisor '{advisor}'")
        
        # Combine prompts: system prompt + advisor prompt
        if advisor_prompt:
            full_prompt = f"{system_prompt}\n\n{advisor_prompt}"
        else:
            full_prompt = system_prompt
        
        logger.info(f"Created prompt for advisor='{advisor}', mode='{mode}' ({len(full_prompt)} chars)")
        return full_prompt
    
    def _create_scoring_prompt(self) -> str:
        """
        [DEPRECATED - no longer used in single-pass]
        Create a minimal prompt for quick dimensional scoring (Pass 1)
        """
        return """Analyze this photograph and score it across all 8 dimensions. 
**OUTPUT ONLY JSON - NO OTHER TEXT.**

Required JSON structure (use ONLY straight quotes, ASCII characters):
{
  "dimensions": [
    {"name": "Composition", "score": 7},
    {"name": "Lighting", "score": 6},
    {"name": "Focus & Sharpness", "score": 8},
    {"name": "Color Harmony", "score": 7},
    {"name": "Subject Isolation", "score": 5},
    {"name": "Depth & Perspective", "score": 6},
    {"name": "Visual Balance", "score": 7},
    {"name": "Emotional Impact", "score": 6}
  ]
}

Provide ONLY the JSON above with your scores. No explanations, no comments."""
    
    def _run_inference(self, image: Image.Image, prompt: str, max_tokens: int = None) -> str:
        """
        Run model inference on image with given prompt.
        Returns the raw text output from the model.
        
        Args:
            image: PIL Image to analyze
            prompt: Text prompt for the model
            max_tokens: Override max_new_tokens (for fast scoring pass)
        """
        # Use chat template for proper image token handling
        messages = [
            {"role": "user", "content": [
                {"type": "image"},
                {"type": "text", "text": prompt}
            ]}
        ]
        
        # Prepare inputs using processor with chat template
        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.processor(
            text=text,
            images=[image],
            padding=True,
            return_tensors="pt"
        )
        
        # Move to device
        if self.device == 'cuda':
            inputs = {k: v.cuda() if hasattr(v, 'cuda') else v for k, v in inputs.items()}
        
        # Build generation config, optionally overriding max_tokens
        gen_config = self.generation_config.copy()
        if max_tokens is not None:
            gen_config['max_new_tokens'] = max_tokens
        
        # Generate response
        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs, 
                **gen_config,
                eos_token_id=self.processor.tokenizer.eos_token_id
            )
        
        # Decode only the generated tokens (exclude input prompt)
        input_length = inputs['input_ids'].shape[1]
        generated_ids = output_ids[:, input_length:]
        
        response = self.processor.batch_decode(
            generated_ids, 
            skip_special_tokens=True
        )[0]
        
        return response
    
    def _parse_scoring_response(self, response: str) -> List[Dict[str, Any]]:
        """
        [DEPRECATED - no longer used in single-pass]
        Parse the quick scoring response (Pass 1) to extract dimensions.
        """
        import re
        
        try:
            # Find JSON in response
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                # Sanitize quotes
                json_str = json_str.replace('"', '"').replace('"', '"')
                data = json.loads(json_str)
                dimensions = data.get('dimensions', [])
                logger.info(f"Pass 1: Parsed {len(dimensions)} dimensional scores")
                return dimensions
        except Exception as e:
            logger.warning(f"Pass 1: Failed to parse scoring response: {e}")
        
        return []
    
    def analyze_image_single_pass(self, image_path: str, advisor: str = "ansel",
                                   mode: str = "rag") -> Dict[str, Any]:
        """
        Single-pass image analysis with RAG.
        Retrieves ALL top references and passages, lets LLM decide which to cite.
        
        Args:
            image_path: Path to image file
            advisor: Photography advisor persona
            mode: Analysis mode (rag modes get full RAG context)
        
        Returns:
            Dictionary with analysis results
        """
        try:
            # Load and validate image
            image = Image.open(image_path).convert('RGB')
            logger.info(f"[Single-Pass] Loaded image: {image_path} ({image.size})")
            
            # =================================================================
            # RETRIEVAL: Get top references and book passages (no weak dim filter)
            # =================================================================
            logger.info("[Single-Pass] === RETRIEVAL: References + Passages ===")
            
            # Get top reference images across ALL dimensions
            reference_images = get_top_reference_images(
                DB_PATH, advisor, max_total=10
            )
            logger.info(f"[Single-Pass] Retrieved {len(reference_images)} reference image candidates")
            
            # Get top book passages across ALL dimensions
            book_passages = []
            try:
                from mondrian.embedding_retrieval import get_top_book_passages
                book_passages = get_top_book_passages(
                    advisor_id=advisor,
                    max_passages=6
                )
                logger.info(f"[Single-Pass] Retrieved {len(book_passages)} quote candidates")
            except Exception as e:
                logger.warning(f"[Single-Pass] Failed to retrieve book passages: {e}")
            
            # =================================================================
            # BUILD PROMPT WITH RAG CONTEXT
            # =================================================================
            logger.info("[Single-Pass] === Building RAG-Augmented Prompt ===")
            
            # Build augmented prompt with all RAG context
            full_prompt = self._create_prompt(advisor, mode)
            full_prompt = self._build_rag_prompt(
                full_prompt, 
                reference_images, 
                book_passages
            )
            
            # =================================================================
            # INFERENCE: Single pass with full context
            # =================================================================
            logger.info(f"[Single-Pass] Prompt: {len(full_prompt)} chars")
            response = self._run_inference(image, full_prompt)
            logger.info(f"[Single-Pass] Response: {len(response)} chars")
            
            # Parse response with citation validation
            analysis = self._parse_response(
                response, advisor, mode, full_prompt, 
                reference_images=reference_images,
                book_passages=book_passages,
                user_image_path=image_path
            )
            
            # Add metadata
            analysis['single_pass'] = True
            analysis['rag_candidates'] = {
                'images': len(reference_images),
                'quotes': len(book_passages)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"[Single-Pass] Error: {e}")
            logger.error(traceback.format_exc())
            # Fall back to basic analysis
            logger.info("[Single-Pass] Falling back to basic analysis")
            return self.analyze_image(image_path, advisor, mode)
    
    # Keep old function name as alias for backwards compatibility
    def analyze_image_two_pass(self, image_path: str, advisor: str = "ansel",
                                mode: str = "rag") -> Dict[str, Any]:
        """
        [DEPRECATED] Alias for analyze_image_single_pass.
        Two-pass has been replaced with single-pass for efficiency.
        """
        return self.analyze_image_single_pass(image_path, advisor, mode)
    
    def _augment_prompt_for_pass2(self, prompt: str, weak_dimensions: List[Dict],
                                   reference_images: List[Dict], 
                                   book_passages: List[Dict]) -> str:
        """
        [DEPRECATED - use _build_rag_prompt for single-pass]
        Augment the full analysis prompt with targeted RAG context for Pass 2.
        Assigns citation IDs to each candidate for LLM to reference.
        """
        rag_context = "\n\n### TARGETED GUIDANCE FOR YOUR WEAKEST AREAS:\n"
        
        weak_names = [d['name'] for d in weak_dimensions]
        rag_context += f"Focus your feedback on these dimensions where improvement is most needed: **{', '.join(weak_names)}**\n"
        
        # Add book passages with citation IDs
        if book_passages:
            rag_context += "\n#### AVAILABLE QUOTES FROM MY WRITINGS:\n"
            rag_context += "You may cite UP TO 3 of these quotes total across all dimensions. Each dimension may cite ONE quote maximum. Never reuse quote IDs.\n\n"
            
            for idx, passage in enumerate(book_passages, 1):
                book_title = passage['book_title']
                text = passage['passage_text']
                dims = passage['dimensions']
                quote_id = f"QUOTE_{idx}"
                
                # Truncate to 75 words for preview
                words = text.split()
                preview = ' '.join(words[:75])
                if len(words) > 75:
                    preview += "..."
                
                rag_context += f"[{quote_id}] From \"{book_title}\" (relevant to: {', '.join(dims)})\n"
                rag_context += f'  "{preview}"\n\n'
            
            rag_context += "**CITATION INSTRUCTION:** To cite a quote, include its ID in your JSON response: `\"quote_id\": \"QUOTE_1\"`\n"
            rag_context += "Cite ONLY when the quote directly supports your specific feedback for that dimension.\n"
        
        # Add reference images with citation IDs
        if reference_images:
            rag_context += "\n#### AVAILABLE REFERENCE IMAGES:\n"
            rag_context += "You may cite UP TO 3 of these images total across all dimensions. Each dimension may cite ONE image maximum. Never reuse image IDs.\n\n"
            
            for idx, img in enumerate(reference_images, 1):
                img_title = img.get('image_title') or img.get('image_path', '').split('/')[-1]
                year = img.get('date_taken', '')
                location = img.get('location', '')
                img_id = f"IMG_{idx}"
                
                # Get all dimension scores >= 8.0
                strong_dims = []
                for dim in ['composition', 'lighting', 'focus_sharpness', 'color_harmony', 
                           'subject_isolation', 'depth_perspective', 'visual_balance', 'emotional_impact']:
                    score = img.get(f"{dim}_score", 0)
                    if score and score >= 8.0:
                        dim_display = dim.replace('_', ' ').title()
                        strong_dims.append(f"{dim_display}={score:.1f}")
                
                rag_context += f"[{img_id}] \"{img_title}\" ({year})\n"
                if location:
                    rag_context += f"  Location: {location}\n"
                rag_context += f"  Strengths: {', '.join(strong_dims)}\n"
                if img.get('image_description'):
                    desc = img['image_description'][:120]
                    rag_context += f"  Description: {desc}...\n"
                rag_context += "\n"
            
            rag_context += "**CITATION INSTRUCTION:** To cite an image, include its ID in your JSON response: `\"case_study_id\": \"IMG_3\"`\n"
            rag_context += "Cite ONLY when the image's strong dimensions match what the user needs to improve.\n"
        
        # Add final reminder about dimension-specific techniques
        rag_context += "\n**CRITICAL CITATION RULES:**\n"
        rag_context += "- Maximum 3 images and 3 quotes total across ALL dimensions\n"
        rag_context += "- Each dimension: cite at most ONE image and ONE quote\n"
        rag_context += "- NEVER reuse an ID once cited in another dimension\n"
        rag_context += "- Only cite if directly relevant to your specific feedback\n"
        rag_context += "- Zone System quotes ONLY for Lighting dimension\n"
        
        return prompt + rag_context
    
    def _build_rag_prompt(self, prompt: str, reference_images: List[Dict], 
                          book_passages: List[Dict]) -> str:
        """
        Build RAG-augmented prompt for single-pass analysis.
        Assigns citation IDs to ALL candidates and lets LLM decide relevance.
        """
        rag_context = "\n\n### REFERENCE MATERIALS AVAILABLE:\n"
        rag_context += "Here are reference images and quotes from my writings. Cite any that are directly relevant to your feedback on specific dimensions.\n"
        
        # Add book passages with citation IDs
        if book_passages:
            rag_context += "\n#### AVAILABLE QUOTES FROM MY WRITINGS:\n"
            rag_context += "You may cite UP TO 3 of these quotes total across all dimensions. Each dimension may cite ONE quote maximum. Never reuse quote IDs.\n\n"
            
            for idx, passage in enumerate(book_passages, 1):
                book_title = passage['book_title']
                text = passage['passage_text']
                dims = passage['dimensions']
                quote_id = f"QUOTE_{idx}"
                
                # Truncate to 75 words for preview
                words = text.split()
                preview = ' '.join(words[:75])
                if len(words) > 75:
                    preview += "..."
                
                rag_context += f"[{quote_id}] From \"{book_title}\" (relevant to: {', '.join(dims)})\n"
                rag_context += f'  "{preview}"\n\n'
            
            rag_context += "**CITATION INSTRUCTION:** To cite a quote, include its ID in your JSON response: `\"quote_id\": \"QUOTE_1\"`\n"
            rag_context += "Cite ONLY when the quote directly supports your specific feedback for that dimension.\n"
        
        # Add reference images with citation IDs
        if reference_images:
            rag_context += "\n#### AVAILABLE REFERENCE IMAGES:\n"
            rag_context += "You may cite UP TO 3 of these images total across all dimensions. Each dimension may cite ONE image maximum. Never reuse image IDs.\n\n"
            
            for idx, img in enumerate(reference_images, 1):
                img_title = img.get('image_title') or img.get('image_path', '').split('/')[-1]
                year = img.get('date_taken', '')
                location = img.get('location', '')
                img_id = f"IMG_{idx}"
                
                # Get all dimension scores >= 8.0
                strong_dims = []
                for dim in ['composition', 'lighting', 'focus_sharpness', 'color_harmony', 
                           'subject_isolation', 'depth_perspective', 'visual_balance', 'emotional_impact']:
                    score = img.get(f"{dim}_score", 0)
                    if score and score >= 8.0:
                        dim_display = dim.replace('_', ' ').title()
                        strong_dims.append(f"{dim_display}={score:.1f}")
                
                rag_context += f"[{img_id}] \"{img_title}\" ({year})\n"
                if location:
                    rag_context += f"  Location: {location}\n"
                if strong_dims:
                    rag_context += f"  Strengths: {', '.join(strong_dims)}\n"
                if img.get('image_description'):
                    desc = img['image_description'][:120]
                    rag_context += f"  Description: {desc}...\n"
                rag_context += "\n"
            
            rag_context += "**CITATION INSTRUCTION:** To cite an image, include its ID in your JSON response: `\"case_study_id\": \"IMG_3\"`\n"
            rag_context += "Cite when the image demonstrates excellence in dimensions where you have feedback.\n"
        
        # Add final reminder about dimension-specific techniques
        rag_context += "\n**CRITICAL CITATION RULES:**\n"
        rag_context += "- Maximum 3 images and 3 quotes total across ALL dimensions\n"
        rag_context += "- Each dimension: cite at most ONE image and ONE quote\n"
        rag_context += "- NEVER reuse an ID once cited in another dimension\n"
        rag_context += "- Only cite if directly relevant to your specific feedback\n"
        rag_context += "- Zone System quotes ONLY for Lighting dimension\n"
        
        return prompt + rag_context
    
    def _get_default_system_prompt(self) -> str:
        """Fallback system prompt if database is unavailable"""
        return """You are a photography analysis assistant. **ALL OUTPUT MUST BE IN ENGLISH ONLY.** Output valid JSON only.
**CRITICAL JSON REQUIREMENTS:**
- Use ONLY straight quotes (") for all strings, NEVER curved/fancy quotes (" or ")
- Use ONLY ASCII characters in all text fields (no Unicode dashes, em-dashes, or special characters)
- Replace special characters: use regular hyphen (-) instead of em-dash ()
- All JSON must be valid and parseable by standard JSON parsers
Required JSON Structure:
{
  "image_description": "2-3 sentence description",
  "dimensions": [
    {"name": "Composition", "score": 8, "comment": "...", "recommendation": "..."},
    {"name": "Lighting", "score": 7, "comment": "...", "recommendation": "..."},
    {"name": "Focus & Sharpness", "score": 9, "comment": "...", "recommendation": "..."},
    {"name": "Color Harmony", "score": 6, "comment": "...", "recommendation": "..."},
    {"name": "Depth & Perspective", "score": 7, "comment": "...", "recommendation": "..."},
    {"name": "Visual Balance", "score": 8, "comment": "...", "recommendation": "..."},
    {"name": "Emotional Impact", "score": 7, "comment": "...", "recommendation": "..."}
  ],
  "overall_score": 7.4,
  "key_strengths": ["strength 1", "strength 2"],
  "priority_improvements": ["improvement 1", "improvement 2"],
  "technical_notes": "Technical observations"
}"""
    
    def _generate_ios_detailed_html(self, analysis_data: Dict[str, Any], advisor: str, mode: str, case_studies: List[Dict[str, Any]] = None) -> str:
        """Generate iOS-compatible dark theme HTML for detailed analysis
        
        Args:
            analysis_data: Parsed analysis from LLM (with _cited_image and _cited_quote in dimensions)
            advisor: Advisor name
            mode: Analysis mode
            case_studies: Deprecated parameter, no longer used (LLM now cites directly)
        """
        
        # Extract data from the expected JSON structure
        image_description = analysis_data.get('image_description', 'Image analysis')
        dimensions = analysis_data.get('dimensions', [])
        overall_score = analysis_data.get('overall_score', 'N/A')
        technical_notes = analysis_data.get('technical_notes', '')
        
        # Format dimension names
        for dim in dimensions:
            if 'name' in dim and dim['name']:
                dim['name'] = format_dimension_name(dim['name'])
        
        def format_dimension_name(name: str) -> str:
            """Format dimension names to have proper spacing (e.g., ColorHarmony -> Color Harmony)"""
            import re
            # Insert space before uppercase letters that follow lowercase letters
            formatted = re.sub(r'([a-z])([A-Z])', r'\1 \2', name)
            return formatted
        
        def get_rating_style(score: int) -> tuple:
            """Return color and rating text based on score"""
            if score >= 8:
                return "#388e3c", "Excellent"  # Green
            elif score >= 6:
                return "#f57c00", "Good"       # Orange
            else:
                return "#d32f2f", "Needs Work" # Red
        
        html = f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', sans-serif;
            padding: 20px;
            background: #000000;
            line-height: 1.6;
            color: #ffffff;
            max-width: 100%;
        }}
        @media (max-width: 768px) {{ body {{ padding: 15px; }} .analysis {{ padding: 15px; }} }}
        @media (max-width: 375px) {{ body {{ padding: 10px; font-size: 14px; }} .analysis {{ padding: 12px; }} }}
        @media (min-width: 1024px) {{ body {{ max-width: 800px; margin: 0 auto; padding: 30px; }} }}
        .analysis {{
            background: #1c1c1e;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 20px;
            text-align: left;
        }}
        .analysis h2 {{
            color: #ffffff;
            font-size: 22px;
            font-weight: 600;
            margin: 0 0 12px 0;
            text-align: left;
        }}
        .analysis p {{
            color: #d1d1d6;
            font-size: 16px;
            margin: 0 0 16px 0;
            line-height: 1.6;
            text-align: left;
        }}
        .feedback-card {{
            background: #fff;
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 8px;
        }}
        .feedback-card h3 {{
            margin-top: 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
            color: #333;
        }}
        .feedback-comment {{
            margin: 15px 0;
            padding: 12px;
            background: #f8f9fa;
            border-radius: 4px;
        }}
        .feedback-comment p {{ margin: 0; line-height: 1.6; color: #333; }}
        .feedback-recommendation {{
            margin-top: 15px;
            padding: 12px;
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            border-radius: 4px;
        }}
        .feedback-recommendation strong {{
            display: block;
            margin-bottom: 8px;
            color: #1976d2;
        }}
        .feedback-recommendation p {{ margin: 0; line-height: 1.6; color: #333; }}
        .reference-citation {{
            margin-top: 16px;
            padding: 0;
            background: transparent;
            border: none;
            border-radius: 0;
            font-size: 14px;
        }}
        .reference-citation .case-study-box {{
            background: #2c2c2e;
            border-radius: 8px;
            padding: 16px;
            border-left: 4px solid #30b0c0;
            overflow: hidden;
        }}
        .reference-citation .case-study-image {{
            width: 100%;
            height: auto;
            max-width: 100%;
            border-radius: 6px;
            margin-bottom: 12px;
            display: block;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
        }}
        .reference-citation .case-study-title {{
            color: #ffffff;
            font-size: 16px;
            margin: 0 0 12px 0;
            font-weight: 600;
        }}
        .reference-citation .case-study-metadata {{
            color: #d1d1d6;
            font-size: 13px;
            line-height: 1.5;
            margin: 8px 0 0 0;
        }}
        .reference-citation strong {{ color: #30b0c0; }}
        .advisor-quote-box {{
            background: #2c2c2e;
            border-radius: 8px;
            padding: 16px;
            border-left: 4px solid #ff9500;
            overflow: hidden;
            margin-top: 12px;
        }}
        .advisor-quote-box .advisor-quote-title {{
            color: #ffffff;
            font-size: 14px;
            margin: 0 0 12px 0;
            font-weight: 600;
        }}
        .advisor-quote-box .advisor-quote-text {{
            color: #d1d1d6;
            font-size: 14px;
            line-height: 1.6;
            font-style: italic;
            margin: 0;
        }}
        .advisor-quote-box .advisor-quote-source {{
            color: #a1a1a6;
            font-size: 12px;
            margin-top: 8px;
            font-style: normal;
        }}
    </style>
</head>
<body>
<div class="advisor-section" data-advisor="{advisor}">
  <div class="analysis">
  <h2>Description</h2>
  <p>{image_description}</p>

  <h2>Improvement Guide</h2>
  <p style="color: #666; margin-bottom: 20px;">Each dimension is analyzed with specific feedback and actionable recommendations for improvement.</p>
'''
        
        # Add dimension cards
        for dim in dimensions:
            name = dim.get('name', 'Unknown')
            score = dim.get('score', 0)
            comment = dim.get('comment', 'No analysis available.')
            recommendation = dim.get('recommendation', 'No recommendation available.')
            color, rating = get_rating_style(score)
            
            # Check if LLM cited an image for this dimension
            cited_image = dim.get('_cited_image')
            image_citation_html = ""
            if cited_image:
                ref_title = cited_image.get('image_title', 'Reference Image')
                ref_year = cited_image.get('date_taken', '')
                ref_path = cited_image.get('image_path', '')
                ref_location = cited_image.get('location', '')
                
                # Format title with year
                if ref_year and str(ref_year).strip():
                    title_with_year = f"{ref_title} ({ref_year})"
                else:
                    title_with_year = ref_title
                
                # Get image data and convert to base64
                ref_image_url = ''
                if ref_path and os.path.exists(ref_path):
                    try:
                        with open(ref_path, 'rb') as img_file:
                            image_data = img_file.read()
                            b64_image = base64.b64encode(image_data).decode('utf-8')
                            img_ext = os.path.splitext(ref_path)[1].lower()
                            mime_type = 'image/png' if img_ext == '.png' else 'image/jpeg' if img_ext in ['.jpg', '.jpeg'] else 'image/png'
                            ref_image_url = f"data:{mime_type};base64,{b64_image}"
                    except Exception as e:
                        logger.warning(f"Failed to embed image as base64: {e}")
                
                # Build case study box
                image_citation_html = '<div class="reference-citation"><div class="case-study-box">'
                image_citation_html += f'<div class="case-study-title">Case Study: {title_with_year}</div>'
                
                if ref_image_url:
                    image_citation_html += f'<img src="{ref_image_url}" alt="{title_with_year}" class="case-study-image" />'
                
                # Add metadata
                metadata_parts = []
                if cited_image.get('image_description'):
                    metadata_parts.append(f'<strong>Description:</strong> {cited_image["image_description"]}')
                if ref_location:
                    metadata_parts.append(f'<strong>Location:</strong> {ref_location}')
                
                # Show dimensional strengths
                dim_strengths = []
                for dim_key in ['composition', 'lighting', 'focus_sharpness', 'color_harmony',
                               'subject_isolation', 'depth_perspective', 'visual_balance', 'emotional_impact']:
                    dim_score = cited_image.get(f"{dim_key}_score", 0)
                    if dim_score and dim_score >= 8.0:
                        dim_display = dim_key.replace('_', ' ').title()
                        dim_strengths.append(f"{dim_display}: {dim_score}/10")
                
                if dim_strengths:
                    metadata_parts.append(f'<strong>Strengths:</strong> {", ".join(dim_strengths)}')
                
                image_citation_html += f'<div class="case-study-metadata">' + '<br/>'.join(metadata_parts) + '</div>'
                image_citation_html += '</div></div>'
                
                logger.info(f"[HTML Gen] Added LLM-cited image for {name}: '{ref_title}'")
            
            # Check if LLM cited a quote for this dimension
            cited_quote = dim.get('_cited_quote')
            quote_citation_html = ""
            if cited_quote:
                book_title = cited_quote.get('book_title', 'Unknown Book')
                passage_text = cited_quote.get('passage_text', cited_quote.get('text', ''))
                quote_dims = cited_quote.get('dimensions', [])
                
                # Truncate to 75 words
                words = passage_text.split()
                truncated_text = ' '.join(words[:75])
                if len(words) > 75:
                    truncated_text += "..."
                
                quote_citation_html = '<div class="advisor-quote-box">'
                quote_citation_html += '<div class="advisor-quote-title">Advisor Insight</div>'
                quote_citation_html += f'<div class="advisor-quote-text">"{truncated_text}"</div>'
                quote_citation_html += f'<div class="advisor-quote-source"><strong>From:</strong> {book_title}</div>'
                quote_citation_html += '</div>'
                
                logger.info(f"[HTML Gen] Added LLM-cited quote for {name} from '{book_title}'")
            
            html += f'''
  <div class="feedback-card">
    <h3>
      <span>{name}</span>
      <span style="color: {color}; font-size: 1.1em;">{score}/10 <span style="font-size: 0.7em; font-weight: normal;">({rating})</span></span>
    </h3>
    <div class="feedback-comment" style="border-left: 4px solid {color};">
      <p>{comment}</p>
    </div>
    <div class="feedback-recommendation">
      <strong>How to Improve:</strong>
      <p>{recommendation}</p>
    </div>{image_citation_html}{quote_citation_html}
  </div>
'''
        
        html += f'''
  <h2>Overall Grade</h2>
  <p><strong>{overall_score}/10</strong></p>
  <p><strong>Grade Note:</strong> {technical_notes}</p>
'''
        
        html += '''
</div>
</div>
</body>
</html>'''
        
        return html
    
    def _generate_summary_html(self, analysis_data: Dict[str, Any]) -> str:
        """Generate iOS-compatible summary HTML with Top 3 recommendations (lowest scoring dimensions)"""
        
        def format_dimension_name(name: str) -> str:
            """Format dimension names to have proper spacing (e.g., ColorHarmony -> Color Harmony)"""
            import re
            # Insert space before uppercase letters that follow lowercase letters
            formatted = re.sub(r'([a-z])([A-Z])', r'\1 \2', name)
            return formatted
        
        dimensions = analysis_data.get('dimensions', [])
        
        # Format dimension names
        for dim in dimensions:
            if 'name' in dim and dim['name']:
                dim['name'] = format_dimension_name(dim['name'])
        
        # Sort by score ascending to get lowest/weakest areas first
        sorted_dims = sorted(dimensions, key=lambda d: d.get('score', 10))[:3]
        
        html = '''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            padding: 16px;
            background: #000000;
            color: #ffffff;
        }
        .summary-header { margin-bottom: 8px; padding-bottom: 16px; }
        .summary-header h1 { font-size: 24px; font-weight: 600; margin-bottom: 8px; }
        .recommendations-list { display: flex; flex-direction: column; gap: 12px; }
        .recommendation-item {
            display: flex;
            gap: 12px;
            padding: 10px;
            background: #1c1c1e;
            border-radius: 6px;
            border-left: 3px solid #30b0c0;
        }
        .rec-number {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 28px;
            height: 28px;
            background: #0a84ff;
            color: #ffffff;
            border-radius: 50%;
            font-weight: 600;
            font-size: 12px;
            flex-shrink: 0;
        }
        .rec-content { flex: 1; }
        .rec-text { font-size: 14px; line-height: 1.4; color: #e0e0e0; }
        .disclaimer {
            margin-top: 24px;
            padding: 16px;
            background: #1c1c1e;
            border-radius: 8px;
            border-left: 3px solid #ff9500;
        }
        .disclaimer p { font-size: 12px; line-height: 1.4; color: #d1d1d6; margin: 0; }
        .advisor-quote-box {{
            background: #2c2c2e;
            border-radius: 8px;
            padding: 12px;
            border-left: 4px solid #ff9500;
            overflow: hidden;
            margin-top: 10px;
        }}
        .advisor-quote-box .advisor-quote-title {{
            color: #ffffff;
            font-size: 12px;
            margin: 0 0 8px 0;
            font-weight: 600;
        }}
        .advisor-quote-box .advisor-quote-text {{
            color: #d1d1d6;
            font-size: 12px;
            line-height: 1.4;
            font-style: italic;
            margin: 0;
        }}
        .advisor-quote-box .advisor-quote-source {{
            color: #a1a1a6;
            font-size: 10px;
            margin-top: 6px;
            font-style: normal;
        }}
    </style>
</head>
<body>
<div class="summary-header"><h1>Top 3 Recommendations</h1></div>
<div class="recommendations-list">
'''
        
        for i, dim in enumerate(sorted_dims, 1):
            name = dim.get('name', 'Unknown')
            score = dim.get('score', 0)
            recommendation = dim.get('recommendation', 'No recommendation available.')
            
            html += f'''  <div class="recommendation-item">
    <div class="rec-number">{i}</div>
    <div class="rec-content">
      <p class="rec-text"><strong>{name}</strong> ({score}/10): {recommendation}</p>
    </div>
  </div>
'''
        
        html += f'''</div>
<div class="disclaimer">
    <p><strong>Note:</strong> {get_disclaimer_text(DB_PATH)}</p>
</div>
</body>
</html>'''
        
        return html
    
    def _generate_advisor_bio_html(self, advisor_data: Dict[str, Any]) -> str:
        """Generate iOS-compatible advisor bio HTML from database"""
        
        name = advisor_data.get('name', 'Unknown Advisor')
        bio = advisor_data.get('bio', 'No biography available.')
        years = advisor_data.get('years', '')
        wikipedia_url = advisor_data.get('wikipedia_url', '')
        commons_url = advisor_data.get('commons_url', '')
        
        html = f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            padding: 20px;
            background: #000000;
            color: #ffffff;
            line-height: 1.6;
        }}
        .advisor-profile {{
            background: #1c1c1e;
            padding: 24px;
            border-radius: 12px;
            margin-bottom: 24px;
        }}
        .advisor-profile h1 {{
            color: #ffffff;
            font-size: 28px;
            font-weight: 600;
            margin-bottom: 8px;
        }}
        .advisor-years {{
            color: #98989d;
            font-size: 16px;
            font-weight: 400;
            margin-bottom: 16px;
        }}
        .advisor-bio {{
            color: #d1d1d6;
            font-size: 16px;
            line-height: 1.6;
            margin-bottom: 16px;
        }}
        .link-button {{
            display: inline-block;
            color: #007AFF;
            text-decoration: none;
            font-size: 16px;
            font-weight: 500;
            padding: 8px 16px;
            border: 1px solid #007AFF;
            border-radius: 6px;
            margin-right: 10px;
            margin-bottom: 10px;
        }}
    </style>
</head>
<body>
<div class="advisor-profile">
    <h1>{name}</h1>
    <p class="advisor-years">{years}</p>
    <p class="advisor-bio">{bio}</p>
'''
        
        if wikipedia_url:
            html += f'    <a href="{wikipedia_url}" class="link-button">Wikipedia</a>\n'
        if commons_url:
            html += f'    <a href="{commons_url}" class="link-button">Wikimedia Commons</a>\n'
        
        html += '''</div>
</body>
</html>'''
        
        return html
    
    def _parse_response(self, response: str, advisor: str, mode: str, prompt: str, 
                        reference_images: List[Dict[str, Any]] = None,
                        book_passages: List[Dict[str, Any]] = None,
                        user_image_path: str = None) -> Dict[str, Any]:
        """Parse model response into structured format with iOS-compatible HTML
        
        Args:
            response: Raw LLM response text
            advisor: Advisor ID (e.g., 'ansel')
            mode: Analysis mode
            prompt: The prompt that was used
            reference_images: Candidate reference images for citation validation
            book_passages: Candidate quotes for citation validation
            user_image_path: Path to user's image for computing visual relevance
        """

        import re
        
        if reference_images is None:
            reference_images = []
        if book_passages is None:
            book_passages = []

        # Extract thinking if present (for thinking models like Qwen3-VL-4B-Thinking)
        thinking_text = ""

        # Check for <thinking> tags (Qwen thinking model format)
        thinking_match = re.search(r'<thinking>(.*?)</thinking>', response, re.DOTALL)
        if thinking_match:
            thinking_text = thinking_match.group(1).strip()
            logger.info(f" Extracted extended thinking ({len(thinking_text)} chars)")
            # Remove thinking tags from response before JSON parsing
            response = re.sub(r'<thinking>.*?</thinking>', '', response, flags=re.DOTALL).strip()

        # Try to extract JSON from response
        analysis_data = {}
        parse_success = False

        # Warn if response is too long (likely runaway generation in LoRA mode)
        if len(response) > 6000:
            logger.warning(f"  Response is unusually long ({len(response)} chars) - may indicate runaway generation")
            # Don't truncate yet - we'll try to extract JSON first

        try:
            # Find JSON in response (handle both raw JSON and markdown-wrapped JSON)
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1

            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]

                # Validate JSON isn't too truncated
                if len(json_str) < 100:
                    logger.warning("  JSON response too short - likely incomplete or malformed")

                # SANITIZE: Replace Unicode quotes and special characters with ASCII equivalents
                # This handles cases where the model generates fancy quotes or dashes
                json_str = json_str.replace('"', '"').replace('"', '"')  # Unicode quotes -> ASCII quotes
                json_str = json_str.replace(''', "'").replace(''', "'")  # Unicode apostrophes -> ASCII apostrophes
                json_str = json_str.replace('', '-').replace('', '-')  # Em-dashes and en-dashes -> hyphen
                json_str = json_str.replace('', '...')  # Ellipsis -> three dots

                analysis_data = json.loads(json_str)
                parse_success = True
                
                # CRITICAL: Check if thinking leaked into image_description
                image_desc = analysis_data.get('image_description', '')
                if any(phrase in image_desc.lower() for phrase in [
                    'okay, let me', 'step by step', 'first,', 'let me analyze', 
                    'i need to', 'the user wants', 'let\'s check', 'wait', 'hold up'
                ]):
                    logger.warning("  Detected thinking contamination in image_description - this indicates prompt/model issue")
                    # If thinking leaked into description, try to extract actual description after reasoning
                    # or mark as unparseable
                    analysis_data['image_description'] = "Unable to parse response - thinking model contaminated output"
                    analysis_data['contamination_detected'] = True
                
                # VALIDATE AND RESOLVE CITATIONS
                dimensions = analysis_data.get('dimensions', [])
                if dimensions:
                    # Build lookup maps for candidates
                    img_lookup = {}
                    for idx, img in enumerate(reference_images, 1):
                        img_id = f"IMG_{idx}"
                        img_lookup[img_id] = img
                    
                    quote_lookup = {}
                    for idx, passage in enumerate(book_passages, 1):
                        quote_id = f"QUOTE_{idx}"
                        quote_lookup[quote_id] = passage
                    
                    # Track used IDs to enforce no-repeat rule
                    used_img_ids = set()
                    used_quote_ids = set()
                    img_citation_count = 0
                    quote_citation_count = 0
                    
                    # Validate citations in each dimension
                    for dim in dimensions:
                        # Validate case_study_id (image citation)
                        if 'case_study_id' in dim:
                            img_id = dim['case_study_id']
                            if img_id in used_img_ids:
                                logger.warning(f" Duplicate image citation: {img_id} in {dim['name']} - removing")
                                del dim['case_study_id']
                            elif img_id not in img_lookup:
                                logger.warning(f" Invalid image citation: {img_id} not in candidates - removing")
                                del dim['case_study_id']
                            elif img_citation_count >= 3:
                                logger.warning(f" Too many image citations (>3): removing {img_id} from {dim['name']}")
                                del dim['case_study_id']
                            else:
                                # Valid citation - mark as used and attach full image data
                                used_img_ids.add(img_id)
                                img_citation_count += 1
                                dim['_cited_image'] = img_lookup[img_id]
                                logger.info(f" Valid image citation: {img_id} in {dim['name']}")
                        
                        # Validate quote_id (quote citation)
                        if 'quote_id' in dim:
                            quote_id = dim['quote_id']
                            if quote_id in used_quote_ids:
                                logger.warning(f" Duplicate quote citation: {quote_id} in {dim['name']} - removing")
                                del dim['quote_id']
                            elif quote_id not in quote_lookup:
                                logger.warning(f" Invalid quote citation: {quote_id} not in candidates - removing")
                                del dim['quote_id']
                            elif quote_citation_count >= 3:
                                logger.warning(f" Too many quote citations (>3): removing {quote_id} from {dim['name']}")
                                del dim['quote_id']
                            else:
                                # Valid citation - mark as used and attach full quote data
                                used_quote_ids.add(quote_id)
                                quote_citation_count += 1
                                dim['_cited_quote'] = quote_lookup[quote_id]
                                logger.info(f" Valid quote citation: {quote_id} in {dim['name']}")
                    
                    logger.info(f"[Citations] Validated: {img_citation_count} images, {quote_citation_count} quotes")
                
                logger.info(f" Successfully parsed JSON response ({len(json_str)} chars)")
            else:
                logger.warning("No JSON object found in response")
                analysis_data = {
                    "image_description": "Unable to parse response",
                    "dimensions": [],
                    "overall_score": 0,
                    "raw_response": response
                }
        except json.JSONDecodeError as e:
            logger.error(f" JSON parsing failed: {e} at position {e.pos}")
            logger.warning(f"   Response length: {len(response)}, Error context: ...{response[max(0,e.pos-50):min(len(response),e.pos+50)]}...")
            analysis_data = {
                "image_description": "Unable to parse response",
                "dimensions": [],
                "overall_score": 0,
                "parse_error": str(e),
                "raw_response": response[:500]  # Keep only first 500 chars of raw response
            }
        
        # Load advisor data from database for bio
        advisor_data = get_advisor_from_db(DB_PATH, advisor)
        
        # Extract weak dimensions from analysis and retrieve targeted reference images
        weak_dimension_indices = []
        weak_dimension_names = []
        dimensions = analysis_data.get('dimensions', [])
        if dimensions and len(dimensions) > 0:
            # Sort by score (ascending) to find weakest
            sorted_dims = sorted(dimensions, key=lambda d: d.get('score', 10))
            # Get the 3 weakest dimensions by index
            for d in sorted_dims[:3]:
                dim_name = d.get('name', '')
                dim_index = get_dimension_index(dim_name)
                if dim_index is not None:
                    weak_dimension_indices.append(dim_index)
                    weak_dimension_names.append(DIMENSIONS[dim_index])  # Use canonical name
            
            if weak_dimension_names:
                logger.info(f"User's weakest dimensions: indices={weak_dimension_indices}, names={weak_dimension_names}")
                # Retrieve reference images that excel in these weak areas
                targeted_refs = get_images_for_weak_dimensions(DB_PATH, advisor, weak_dimension_names, max_images=4)
                if targeted_refs:
                    reference_images = targeted_refs
                    logger.info(f"Retrieved {len(reference_images)} targeted reference images for weak dimensions")
                
                # Retrieve book passages for weak dimensions
                try:
                    from mondrian.embedding_retrieval import get_book_passages_for_dimensions
                    passages = get_book_passages_for_dimensions(
                        advisor_id=advisor,
                        weak_dimensions=weak_dimension_names,  # Use canonical names
                        max_passages=2  # User constraint: max 1-2 passages
                    )
                    
                    if passages:
                        # Add passages to analysis_data for inclusion in response
                        analysis_data['book_passages'] = []
                        for passage in passages:
                            analysis_data['book_passages'].append({
                                'book_title': passage['book_title'],
                                'text': passage['passage_text'],
                                'dimensions': passage['dimensions'],
                                'relevance_score': passage['relevance_score']
                            })
                        logger.info(f"Added {len(passages)} book passages to analysis response")
                except Exception as e:
                    logger.warning(f"Failed to retrieve book passages: {e}")
        
        # Compute case studies using gap-based selection with visual relevance
        # This replaces the old reference_images approach with smarter selection
        case_studies = []
        if dimensions and user_image_path:
            case_studies = self._compute_case_studies(
                advisor_id=advisor,
                user_dimensions=dimensions,
                user_image_path=user_image_path,
                max_case_studies=3,
                relevance_threshold=0.25
            )
            if case_studies:
                logger.info(f" Computed {len(case_studies)} case studies for weak dimensions")
        elif dimensions:
            # No user image path - fall back to gap-only selection (no relevance filtering)
            case_studies = self._compute_case_studies(
                advisor_id=advisor,
                user_dimensions=dimensions,
                user_image_path=None,
                max_case_studies=3,
                relevance_threshold=0.0  # No relevance filter when no user image
            )
            if case_studies:
                logger.info(f" Computed {len(case_studies)} case studies (gap-only, no user image)")
        
        # Generate HTML outputs
        analysis_html = self._generate_ios_detailed_html(analysis_data, advisor, mode, case_studies=case_studies)
        summary_html = self._generate_summary_html(analysis_data)
        
        # Generate advisor bio HTML from database
        if advisor_data:
            advisor_bio_html = self._generate_advisor_bio_html(advisor_data)
            advisor_bio = advisor_data.get('bio', f"Analysis by {advisor.title()}")
        else:
            advisor_bio_html = f"<html><body><p>Analysis by {advisor.title()}</p></body></html>"
            advisor_bio = f"Analysis by {advisor.title()}"
        
        # Extract text summary from image_description
        summary = analysis_data.get('image_description', response[:500])
        
        # Calculate overall score if not provided
        overall_score = analysis_data.get('overall_score', None)
        if overall_score is None and 'dimensions' in analysis_data:
            dimensions = analysis_data.get('dimensions', [])
            if dimensions:
                scores = [d.get('score', 0) for d in dimensions]
                overall_score = sum(scores) / len(scores) if scores else 0
        
        # Wrap in standard response format matching expected job service fields
        result = {
            "advisor": advisor,
            "mode": mode,
            "model": self.model_name,
            "adapter": self.adapter_path if self.adapter_path else None,
            "device": self.device,
            "timestamp": datetime.now().isoformat(),
            "prompt": prompt,  # System+advisor prompt sent to LLM
            "llm_prompt": prompt,  # For compatibility
            "full_response": response,  # Complete LLM response (with thinking tags if present)
            "llm_thinking": thinking_text,  # Extracted thinking from thinking models (empty if not present)
            "analysis": analysis_data,  # Structured JSON data
            "analysis_html": analysis_html,  # Detailed HTML for iOS WebView
            "summary_html": summary_html,  # Top 3 recommendations HTML
            "advisor_bio_html": advisor_bio_html,  # Advisor biography HTML
            "summary": summary,  # Text summary
            "advisor_bio": advisor_bio,  # Text bio
            "overall_score": overall_score,  # Numeric score
            "parse_success": parse_success  # Whether JSON parsing succeeded
        }
        
        logger.info(f"Response parsed: {len(analysis_data.get('dimensions', []))} dimensions, score={overall_score}")
        
        return result


# Flask app setup
app = Flask(__name__)
CORS(app)

# Global advisor instance
advisor = None

# Loading status tracking
loading_status = {
    'started': False,
    'completed': False,
    'error': None,
    'progress': 0,
    'message': 'Not started'
}

def init_advisor(model_name: str, load_in_4bit: bool, adapter_path: Optional[str] = None, generation_config: Optional[Dict] = None, backend: str = 'bnb'):
    """Initialize the advisor service"""
    global advisor, loading_status
    try:
        loading_status['started'] = True
        loading_status['message'] = f'Loading model {model_name}...'
        loading_status['progress'] = 10
        
        advisor = QwenAdvisor(
            model_name=model_name,
            load_in_4bit=load_in_4bit,
            adapter_path=adapter_path,
            generation_config=generation_config,
            backend=backend
        )
        
        loading_status['completed'] = True
        loading_status['progress'] = 100
        loading_status['message'] = 'Model ready'
        logger.info("Advisor service initialized successfully")
    except Exception as e:
        loading_status['error'] = str(e)
        loading_status['completed'] = True
        logger.error(f"Failed to initialize advisor: {e}")
        raise


@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint - returns status even while loading"""
    if advisor:
        health_response = {
            "status": "UP",
            "model": advisor.model_name,
            "device": advisor.device,
            "using_gpu": advisor.device == 'cuda',
            "loading": False,
            "timestamp": datetime.now().isoformat()
        }
        
        # Add LoRA adapter info if loaded
        if advisor.adapter_path:
            from pathlib import Path
            adapter_path = Path(advisor.adapter_path)
            health_response["fine_tuned"] = True
            health_response["lora_path"] = str(advisor.adapter_path)
            health_response["adapter_exists"] = adapter_path.exists()
        else:
            health_response["fine_tuned"] = False
            health_response["lora_path"] = None
        
        return jsonify(health_response), 200
    elif loading_status['error']:
        return jsonify({
            "status": "error",
            "error": loading_status['error'],
            "loading": False,
            "timestamp": datetime.now().isoformat()
        }), 503
    else:
        # Still loading
        return jsonify({
            "status": "loading",
            "progress": loading_status['progress'],
            "message": loading_status['message'],
            "loading": True,
            "timestamp": datetime.now().isoformat()
        }), 202


@app.route('/model-status', methods=['GET'])
def model_status():
    """Get model and device status"""
    if loading_status['error']:
        return jsonify({
            "status": "error",
            "error": loading_status['error'],
            "progress": 0
        }), 503
    
    if not advisor:
        # Still loading
        return jsonify({
            "status": "loading",
            "progress": loading_status['progress'],
            "message": loading_status['message']
        }), 202
    
    return jsonify({
        "status": "ready",
        "model": advisor.model_name,
        "device": advisor.device,
        "using_gpu": advisor.device == 'cuda',
        "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3) if advisor.device == 'cuda' else None,
        "gpu_memory_used": torch.cuda.memory_allocated(0) / (1024**3) if advisor.device == 'cuda' else None,
        "timestamp": datetime.now().isoformat()
    }), 200


@app.route('/analyze', methods=['POST'])
def analyze():
    """Analyze an image"""
    if not advisor:
        return jsonify({"error": "Service not initialized"}), 503
    
    try:
        # Get image from request
        if 'image' not in request.files:
            return jsonify({"error": "No image provided"}), 400
        
        image_file = request.files['image']
        
        # Save temporarily
        temp_path = f"/tmp/{image_file.filename}"
        image_file.save(temp_path)
        
        # Get parameters
        advisor_name = request.form.get('advisor', 'ansel')
        mode_str = request.form.get('mode', 'baseline')
        
        # Use single-pass RAG analysis for RAG modes
        use_rag = mode_str in ('rag', 'rag_lora', 'lora+rag', 'lora_rag')
        
        # Run analysis
        if use_rag:
            logger.info(f"[Single-Pass RAG] Analyzing image with advisor={advisor_name}, mode={mode_str}")
            result = advisor.analyze_image_single_pass(temp_path, advisor=advisor_name, mode=mode_str)
        else:
            logger.info(f"Analyzing image with advisor={advisor_name}, mode={mode_str}")
            result = advisor.analyze_image(temp_path, advisor=advisor_name, mode=mode_str)
        
        # Clean up
        Path(temp_path).unlink()
        
        return jsonify(result), 200
        
    except Exception as e:
        logger.error(f"Analysis error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500


@app.route('/analyze_stream', methods=['POST'])
def analyze_stream():
    """Analyze an image with streaming output (Server-Sent Events)"""
    if not advisor:
        return jsonify({"error": "Service not initialized"}), 503
    
    try:
        from transformers import TextIteratorStreamer
        
        # Get image from request
        if 'image' not in request.files:
            return jsonify({"error": "No image provided"}), 400
        
        image_file = request.files['image']
        
        # Save temporarily
        temp_path = f"/tmp/{image_file.filename}"
        image_file.save(temp_path)
        
        # Get parameters
        advisor_name = request.form.get('advisor', 'ansel')
        mode_str = request.form.get('mode', 'baseline')
        
        logger.info(f"Starting STREAMING analysis with advisor={advisor_name}, mode={mode_str}")
        
        def generate():
            """Generator function for Server-Sent Events"""
            try:
                # Load and validate image
                image = Image.open(temp_path).convert('RGB')
                
                # Create analysis prompt
                prompt = advisor._create_prompt(advisor_name, mode_str)
                
                # Use chat template for proper image token handling
                messages = [
                    {"role": "user", "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt}
                    ]}
                ]
                
                # Prepare inputs
                text = advisor.processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                inputs = advisor.processor(
                    text=text,
                    images=[image],
                    padding=True,
                    return_tensors="pt"
                )
                
                # Move to device
                if advisor.device == 'cuda':
                    inputs = {k: v.cuda() if hasattr(v, 'cuda') else v for k, v in inputs.items()}
                
                # Create streamer
                streamer = TextIteratorStreamer(
                    advisor.processor.tokenizer,
                    skip_special_tokens=True,
                    skip_prompt=True
                )
                
                # Generation parameters
                generation_kwargs = {
                    **inputs,
                    "streamer": streamer,
                    "max_new_tokens": 800,
                    "repetition_penalty": 1.5,
                    "do_sample": True,
                    "temperature": 0.5,
                    "top_p": 0.90,
                    "eos_token_id": advisor.processor.tokenizer.eos_token_id
                }
                
                # Start generation in background thread
                thread = threading.Thread(target=advisor.model.generate, kwargs=generation_kwargs)
                thread.start()
                
                # Send initial event
                yield f"data: {json.dumps({'type': 'start', 'advisor': advisor_name, 'mode': mode_str})}\n\n"
                
                # Stream tokens as they arrive WITH SEPARATION
                full_response = ""
                in_thinking = False
                in_json = False
                thinking_buffer = ""
                json_buffer = ""
                
                for new_text in streamer:
                    full_response += new_text
                    
                    # Detect thinking tags
                    if '<thinking>' in new_text:
                        in_thinking = True
                        yield f"data: {json.dumps({'type': 'thinking_start'})}\n\n"
                        # Remove tag from text
                        new_text = new_text.replace('<thinking>', '')
                    
                    if '</thinking>' in new_text:
                        in_thinking = False
                        yield f"data: {json.dumps({'type': 'thinking_end', 'full_thinking': thinking_buffer})}\n\n"
                        # Remove tag from text
                        new_text = new_text.replace('</thinking>', '')
                    
                    # Detect JSON start
                    if '{' in new_text and not in_thinking and not in_json:
                        in_json = True
                        yield f"data: {json.dumps({'type': 'json_start'})}\n\n"
                    
                    # Send appropriate event type based on current state
                    if new_text.strip():  # Only send non-empty tokens
                        if in_thinking:
                            thinking_buffer += new_text
                            yield f"data: {json.dumps({'type': 'thinking_token', 'text': new_text})}\n\n"
                        elif in_json:
                            json_buffer += new_text
                            yield f"data: {json.dumps({'type': 'json_token', 'text': new_text})}\n\n"
                        else:
                            # Unknown content (shouldn't happen with proper format)
                            yield f"data: {json.dumps({'type': 'token', 'text': new_text})}\n\n"
                
                thread.join()
                
                # Parse final response
                result = advisor._parse_response(
                    full_response, advisor_name, mode_str, prompt,
                    user_image_path=temp_path
                )
                
                # Send complete event with parsed data
                yield f"data: {json.dumps({'type': 'complete', 'result': result})}\n\n"
                
                # Clean up
                Path(temp_path).unlink()
                
            except Exception as e:
                logger.error(f"Streaming error: {e}")
                yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
        
        # Return SSE response
        return app.response_class(
            generate(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        logger.error(f"Stream setup error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.errorhandler(500)
def handle_error(e):
    """Handle internal server errors"""
    logger.error(f"Server error: {e}")
    return jsonify({"error": "Internal server error"}), 500


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='AI Advisor Service for Linux CUDA')
    parser.add_argument('--port', type=int, default=5100, help='Service port')
    parser.add_argument('--model', default='Qwen/Qwen3-VL-4B-Instruct', help='Model to use')
    parser.add_argument('--adapter', default='adapters/ansel_qwen3_4b_v2/epoch_20', help='Path to LoRA adapter')
    parser.add_argument('--load_in_4bit', action='store_true', help='Use 4-bit quantization')
    parser.add_argument('--load_in_8bit', action='store_true', help='Use 8-bit quantization')
    parser.add_argument('--backend', default='bnb', choices=['bnb', 'vllm', 'awq'], help='Inference backend: bnb (BitsAndBytes, default), vllm (vLLM), awq (AutoAWQ)')
    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')
    parser.add_argument('--debug', action='store_true', help='Run in debug mode')
    parser.add_argument('--generation-profile', default=None, help='Generation profile from model_config.json (fast_greedy, beam_search, sampling)')
    
    args = parser.parse_args()
    
    # Load generation config from model_config.json
    generation_config = None
    config_path = Path(__file__).parent.parent / 'model_config.json'
    if config_path.exists():
        try:
            with open(config_path) as f:
                config = json.load(f)
            
            # Use specified profile or try to get from model preset
            profile_name = args.generation_profile or config.get('defaults', {}).get('generation_profile', 'fast_greedy')
            if profile_name in config.get('generation_profiles', {}):
                generation_config = config['generation_profiles'][profile_name]
                logger.info(f"Loaded generation profile '{profile_name}' from model_config.json")
            else:
                logger.warning(f"Generation profile '{profile_name}' not found in model_config.json")
        except Exception as e:
            logger.warning(f"Could not load model_config.json: {e}")
    
    # Log startup info
    logger.info("Starting AI Advisor Service")
    logger.info(f"Port: {args.port}")
    logger.info(f"Model: {args.model}")
    logger.info(f"Adapter: {args.adapter}")
    logger.info(f"4-bit quantization: {args.load_in_4bit}")
    if generation_config:
        logger.info(f"Generation config: {generation_config}")
    
    # Start Flask server in a background thread BEFORE loading the model
    # This ensures the service responds to health checks while loading
    def run_flask():
        logger.info(f"Flask server starting on {args.host}:{args.port}")
        app.run(host=args.host, port=args.port, debug=args.debug, use_reloader=False)
    
    flask_thread = threading.Thread(target=run_flask, daemon=True)
    flask_thread.start()
    
    # Give Flask a moment to start listening
    time.sleep(2)
    
    # NOW load the model in the main thread
    try:
        logger.info("Loading model (this may take several minutes)...")
        init_advisor(args.model, args.load_in_4bit, adapter_path=args.adapter, generation_config=generation_config, backend=args.backend)
        
        # Keep the main thread alive
        flask_thread.join()
    except Exception as e:
        logger.error(f"Fatal error during initialization: {e}")
        logger.error(traceback.format_exc())
        # Flask will still respond with error status
        flask_thread.join()


if __name__ == '__main__':
    main()
